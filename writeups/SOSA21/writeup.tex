\documentclass[11pt]{article}
% \usepackage[letterpaper]{geometry}
\usepackage[margin=1in]{geometry}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{dsfont}
\def\E{\operatorname{\mathbb{E}}}
\usepackage{amssymb}
\usepackage{todonotes}
\usepackage[noend]{algpseudocode}
\newcommand{\dec}{\operatorname{dec}}
\newcommand{\poly}{\operatorname{poly}}
\newcommand{\polylog}{\operatorname{polylog}}
\newcommand{\defn}[1]{{\textit{\textbf{\boldmath #1}}}}
\renewcommand{\paragraph}[1]{\vspace{0.09in}\noindent{\bf \boldmath #1.}}
\usepackage{url}
\usepackage{authblk}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{clm}[thm]{Claim}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{ex}[thm]{Example}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[thm]{Definition}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{claim}[thm]{Claim}
\newtheorem{corollary}[thm]{Corollary}
\newtheorem{conjecture}[thm]{Conjecture}
\theoremstyle{remark}
\newtheorem{remark}[thm]{Remark}
\newtheorem{example}[thm]{Example}
\newtheorem{observation}[thm]{Observation}

\usepackage{enumitem}
\usepackage{subcaption}

\usepackage{tikz,pgfplots}
\usepackage{etoolbox}
%% This makes the colors annoyingly bright, but at least they're easy to distinguish.
\pgfplotsset{
  every  tick/.style={red,}, minor x tick num=1,
  cycle list={teal,every mark/.append style={fill=teal!80!black},mark=*\\%
orange,every mark/.append style={fill=orange!80!black},mark=square*\\%
cyan!60!black,every mark/.append style={fill=cyan!80!black},mark=otimes*\\%
red!70!white,mark=star\\%
lime!80!black,every mark/.append style={fill=lime},mark=diamond*\\%
red,densely dashed,every mark/.append style={solid,fill=red!80!black},mark=*\\%
yellow!60!black,densely dashed,
every mark/.append style={solid,fill=yellow!80!black},mark=square*\\%
black,every mark/.append style={solid,fill=gray},mark=otimes*\\%
blue,densely dashed,mark=star,every mark/.append style=solid\\%
red,densely dashed,every mark/.append style={solid,fill=red!80!black},mark=diamond*\\%
}
}

\input{paralleltable.tex}
\input{serialtable.tex}

%%%

\begin{document}

\title{\Large In-Place Parallel-Partition Algorithms \\using
Exclusive-Read-and-Write Memory}
\author[1]{William Kuszmaul\thanks{Supported by a Hertz
Fellowship and a NSF GRFP Fellowship.}}
\author[1]{Alek Westover\thanks{Supported by Massachusetts Institute of Technology}}
\affil[ ]{\textit{kuszmaul@mit.edu, alek.westover@gmail.com}}
\affil[1]{Massachusetts Institute of Technology}

\date{}

\maketitle
\begin{abstract} 
  We present a simple in-place algorithm for the parallel partition
  problem that has linear work and polylogarithmic span. The algorithm
  uses only exclusive read/write shared memory variables, and can be
  implemented using parallel-for-loops without any additional
  concurrency considerations. A key feature of the algorithm is that
  it exhibits provably optimal cache behavior, up to small-order
  factors.

  We also present a second in-place EREW algorithm that has linear
  work and span $O(\log n \cdot \log \log n)$. This second algorithm
  has nearly optimal span but does not exhibit optimal cache behavior.

  By using this low-span algorithm as a subroutine within the
  cache-friendly algorithm, we are able to obtain a single algorithm
  that combines their theoretical guarantees: the algorithm achieves
  span $O(\log n \cdot \log \log n)$ and optimal cache behavior. As an
  immediate consequence, we also get an in-place quicksort algorithm
  with work $O(n \log n)$, span $O(\log^2 n \cdot \log \log n)$.
\end{abstract}

\clearpage

\section{Introduction}

A \defn{partition} operation rearranges the elements in an array so
that the elements satisfying a particular \defn{pivot property} appear
first. In addition to playing a central role in quicksort, the
partition operation is used (often implicitly) as a primitive
throughout both sequential and parallel algorithms\footnote{In several
  well-known textbooks and surveys on parallel algorithms
  \cite{AcarBl16,Blelloch96}, for example, parallel partitions are
  implicitly used extensively to perform what are referred to as
  \emph{filter} operations.}, and is arguably one of the simplest and
most basic computational operations.

The partition operation is straightforward to implement efficiently
(and in-place) in serial. Designing fast parallel algorithms is more
difficult, however. Although there has been a great deal of empirical
work on parallel partitioning (see, e.g., \cite{HeidelbergerNo90,
  AxtmannWi17, TsigasZh03, FrancisPa92, Frias08}), several fundamental
theoretical questions remain unanswered.

In this paper we consider the following basic question: does there
exist an \emph{in-place} parallel algorithm for the partition
operation? And, more generally, does there exist a parallel algorithm
with optimal cache behavior?

\paragraph{The Classic Sum-and-Swap Algorithm}
A parallel algorithm can be measured by its \defn{work}, the time
needed to execute in serial, and its \defn{span}, the time to execute
on infinitely many processors. A classic algorithm for parallel
partitioning is the \defn{Sum-and-Swap Algorithm}, which achieves
optimal bounds of $O(n)$ work and $O(\log n)$ span
\cite{Blelloch96,AcarBl16}. 

The Sum-and-Swap Algorithm has several appealing properties. The
algorithm can be implemented in the recursive fork-join parallel
programming model \cite{BlumofeLe99, FrigoLeRa98, AroraBlPl98,
  BlumofeJo96, CLRS, FengLe99, Cilkmem, ForkJoin1, Forkjoin2}, and uses
parallel for loops as its only concurrency mechanism. This means that
the algorithm is straightforward to implement in languages such as
Cilk \cite{FrigoLeRa98, Leiserson10, IntelCilkPlus10}, Fortress
\cite{AllenChHa+08}, and OpenMP \cite{OpenMP08, AyguadeCoDu09}, and
that randomized work-stealing schedulers \cite{BlumofeLe99,
  FrigoLeRa98, AroraBlPl98, BlumofeJo96} can execute the algorithm
with provably good parallelism.

Additionally, the Sum-and-Swap Algorithm is \defn{race-free}
\cite{FengLe99, NetzerMi92}, meaning that whenever a processor $p$ is
writing to a memory address, it is guaranteed that no other processor
$p'$ is trying to concurrently read or write the same address. In
addition to being an appealing theoretical property, being race-free
ensures that every program execution on a given input performs the
same set of operations, regardless of scheduling, which makes test
coverage, debugging, and reasoning about performance substantially
easier \cite{BlellochFi12}.

The Sum-and-Swap Algorithm is \emph{not} in-place, however, and
requires large auxiliary arrays for both the Summing and Swapping
phases of the algorithm. Additionally, the Sum-and-Swap Algorithm
requires multiple passes over the input array, causing further cache
inefficiency.

\paragraph{Why Cache Efficiency Matters}
In addition to being of theoretical interest, the problem of designing
in-place algorithms for parallel partitioning is also of practical
importance.

One of the most widely used applications of the partition operation is
in sorting. In library implementations of sorting, an in-place
algorithm is needed to handle cases where there is not enough free
memory in the system to allocate auxiliary arrays \cite{gnu,
  openbsd}. One consequence of this is that, even sorting
implementations that use mergesort as their baseline algorithm often
resort to quicksort as a backup when memory allocation fails (see,
e.g., \cite{gnu}). But parallel quicksort requires the parallel
partition operation, which until now has not yielded to in-place
race-free parallel algorithms with theoretical guarantees on span.

The problem of designing cache-efficient algorithms, in general, is
important for practical performance. This is especially true for
parallel algorithms, since multiple processors may share a single
memory bus which can cause parallel algorithms to become
memory-bandwidth limited even when their sequential counterparts were
not (see, e.g., discussion in \cite{DeakinPrMa18, NiDME09}).

\paragraph{Our Results}
We give an in-place parallel-partition algorithm called the
\defn{Smoothed Striding Algorithm}, with linear work and
polylogarithmic span. Additionally, the algorithm exhibits provably
optimal cache behavior up to low-order terms. In particular, if the
input consists of $m$ cache lines, then the algorithm incurs at most
$m(1 + o(1))$ cache misses, with high probability in $m$. The Smoothed
Striding Algorithm is both simple and straightforward to implement,
and has the potential to be useful in practice.

We also develop a suite of techniques for transforming the
Sum-and-Swap Algorithm into an in-place algorithm. The new algorithm,
which we call the \defn{In-Place Sum-and-Swap Algorithm}, has work
$O(n)$ and span $O(\log n \log \log n)$, which is within a
$\log \log n$ factor of optimal (and better than the polylogarithmic
span of the Smoothed Striding Algorithm). The In-Place Sum-and-Swap
Algorithm does not exhibit optimal cache behavior, however.

Finally, by considering a hybrid of two algorithms, we are able to
achieve the best of both worlds: we obtain a single algorithm with
work $O(n)$, span $O(\log n \log \log n)$ (assuming a cache-line size
of $O(1)$) and nearly optimal cache behavior. As an immediate
consequence, we also get an in-place and cache-efficient quicksort
algorithm with work $O(n \log n)$ and span $O(\log^2 n \log \log n)$.

Like the original Sum-and-Swap Algorithm, all of our algorithms are
fork-join parallel, can be implemented using standard parallel for
loops, and are race free.

\paragraph{Related Work}
Our core algorithm, the Smoothed Striding Algorithm, borrows much of
its basic structure from a previous algorithm, known as the
\defn{Strided Algorithm} \cite{FrancisPa92, Frias08}. The Strided
Algorithm has near optimal cache behavior in practice, but exhibits
theoretical guarantees on span and cache behavior only on certain
\emph{random} input arrays. In contrast, the Smoothed Striding
Algorithm achieves theoretical guarantees on all of work, span, and
cache-optimality. This is achieved by randomly perturbing the internal
structure of the Strided Algorithm, and adding a recursion step that
was previously not possible. These random perturbations are
reminiscent of smoothed analysis \cite{Smoothed1, Smoothed2,
  Smoothed3, Smoothed4, Smoothed5, Smoothed6}, hence of the name of our
algorithm. However, whereas classical smoothed analysis is performed
by randomly perturbing the \emph{input} to an algorithm, and then
analyzing the performance on the perturbed input, our application
instead randomly perturbs the \emph{algorithm}, and analyzes the
algorithm on a worst-case input.

In addition to theoretical work, there has been substantial empirical
work on developing efficient implementations of parallel partitions
\cite{HeidelbergerNo90, AxtmannWi17, TsigasZh03, FrancisPa92,
  Frias08}. Often, practical implementations of parallel partition
leverage concurrency mechanisms such as locks or atomic operations
(see, e.g., \cite{HeidelbergerNo90,AxtmannWi17,TsigasZh03}). Such
mechanisms have the benefit of offering substantial additional power
to the programmer, but also have several drawbacks, making it
difficult to reason about the scalability of an algorithm, the
performance of the underlying scheduler, and the interactions between
caches (e.g., elements of one processor's cache being invalidated due
to writes in another).

The parallel partition problem is closely related to parallel
sorting. Currently the only practical in-place parallel sorting
algorithms either rely heavily on concurrency mechanisms such as
atomic operations \cite{HeidelbergerNo90, AxtmannWi17, TsigasZh03}, or
abstain from theoretical guarantees \cite{FrancisPa92}. Parallel merge
sort \cite{Hagerup89} was made in-place by Katajainen
\cite{Katajainen93}, but has proven too sophisticated for practical
applications. Bitonic sort \cite{BlellochLe98} is naturally in-place,
and can be practical in certain applications on super computers, but
suffers in general from requiring work $\Theta(n \log^2 n)$ rather
than $O(n \log n)$. Parallel quicksort, on the other hand, despite the
many efforts to optimize it \cite{HeidelbergerNo90, AxtmannWi17,
  TsigasZh03, FrancisPa92, Frias08}, has eluded any in-place race-free
algorithms due to its reliance on parallel partition. Our algorithms
suggest one natural (and potentially practical) way to make parallel
quicksort in-place.

The general problem of designing parallel algorithms with good cache
behavior has been studied for many problems
\cite{blelloch2020improved, ParallelCacheAlg1, ParallelCacheAlg2,
  ParallelCacheAlg3, ParallelCacheAlg4, ParallelCacheAlg5,
  ParallelCacheAlg6} besides parallel partition. The problem is also
closely related to the study of of parallel algorithms on asymmetric
and persistent memories (see, e.g.,
\cite{blelloch2018parallel,ben2016parallel,blelloch2015sorting,gu2018survey,blelloch2018parallel2},
or the survey by Gu \cite{gu2018survey}), and exploring parallel
partition further in these contexts is an interesting direction for
future work.


% \paragraph{Outline} We begin in Section \ref{secprelim} by
% discussing background on parallel algorithms and the Parallel
% Partition Problem. In Section~\ref{sec:recursiveSmoothedStriding}
% we present the Smoothed Striding Algorithm and analyze a simple
% version of the Smoothed Striding Algorithm which recursively
% calls itself: the Recursive Smoothed Striding Algorithm. In
% Section \ref{sec:blockedprefixsumpartitionalg} we present and
% analyze the Blocked-Prefix-Sum Partition Algorithm. The
% Blocked-Prefix-Sum Partition Algorithm achieves a nearly optimal
% span of $O(\log n \log \log n)$ but is not cache-optimal. In
% Section~\ref{sec:hybridSmoothedStriding} we show that by using
% the Blocked-Prefix-Sum Partition Algorithm as a subroutine within
% the Smoothed Striding Algorithm we get an algorithm that
% simultaneously attains provably optimal cache behavior and span
% $O(\log n \log \log n)$.

\section{Preliminaries}\label{secprelim}

We begin by describing the parallelism and memory model used in
the paper, and by presenting background on the parallel partition problem.

\paragraph{Fork-Join Parallelism} In this paper we study parallel programs
in the recursive fork-join programming model \cite{BlumofeLe99,
  FrigoLeRa98, AroraBlPl98, BlumofeJo96, CLRS, FengLe99, Cilkmem,
  ForkJoin1, Forkjoin2}. Following conventions similar to past work
\cite{Blelloch96,AcarBl16,CLRS}, we express the parallelism of our
algorithms in terms of \defn{parallel-for-loops}; function calls
within the inner loop then allow for more complicated recursive
fork-join parallel structures. Our algorithms can also be implemented
in the CREW PRAM model \cite{Blelloch96, AcarBl16}.

Formally, a parallel-for-loop is given a range size $R \in \mathbb{N}$, a
constant number of arguments $\arg_1, \arg_2, \ldots, \arg_c$, and a
body of code. For each $i \in \{1, \ldots, R\}$, the loop launches a
thread that is given loop-counter $i$ and local copies of the
arguments $\arg_1, \arg_2, \ldots, \arg_c$. The threads are then taken up by
processors and the iterations of the loop are performed in parallel. Only after
every iteration of the loop is complete can control flow continue past the
loop.

A parallel algorithm may be run on an arbitrary number $p$ of
processors. The algorithm itself is oblivious to $p$, however, leaving
the assignment of threads to processors up to a scheduler.

The \defn{work} $T_1$ of an algorithm is the time that the algorithm
would require to execute on a single processor. The \defn{span}
$T_\infty$ of an algorithm is the time to execute on infinitely many
processors. The scheduler is assumed to contribute no overhead to the
span. In particular, if each iteration of a
parallel-for-loop has span $s$, then the full parallel loop has span
$s + O(1)$ \cite{Blelloch96,AcarBl16}.

The work $T_1$ and span $T_\infty$ can be used to quantify the time $T_p$
that an algorithm requires to execute on $p$ processors using a greedy
online scheduler. If the scheduler is assumed to contribute no
overhead, then Brent's Theorem \cite{Brent74} states that for any
$p$,
$$\max(T_1 / p, T_\infty) \le T_p \le T_1 / p + T_\infty.$$

The work-stealing algorithms used in the Cilk extension of C/C++
realize the guarantee offered by Brent's Theorem within a
constant factor \cite{BlumofeJo96,BlumofeLe99}, with the added
caveat that parallel-for-loops typically induce an additional
additive overhead of $O(\log R)$. 

\paragraph{Race-Free Algorithms}
In this paper, we require algorithms to be \defn{race free}, meaning
that no processor $p$ ever reads/writes a memory address being
concurrently written by another processor $p'$. Note that processors
are not in lockstep (i.e., they may progress at arbitrary different
speeds), and the property of being race-free must hold for any
possible parallel execution.

\paragraph{In-Place Algorithms}
In order for an algorithm to be in-place, it must not rely on large
auxiliary arrays. Formally, in an in-place algorithm, each thread is
given $O(\polylog n)$ memory upon creation that is deallocated when
the thread dies.\footnote{Note that $\Omega(\log n)$ memory is
  necessary just for a thread to store its stack in a
  divide-and-conquer recursion.} This memory can be shared with other
threads via pointers.

\paragraph{Modeling Cache Behavior}
For sequential algorithms, cache behavior can be modeled using the
\defn{External Memory Model} \cite{Vitter08, AggarwalVi88}. The
External Memory Model (sometimes also called the Disk-Access Model)
treats memory as consisting of $M$ fixed-size cache lines, each of
some size $b$. A cache miss occurs in a cache whenever the line being
accessed is not currently in cache, in which case some other line is
evicted from cache to make room for the new entry\footnote{As is
  standard, we will make the simplifying assumption that all
  fixed-size (i.e., non-array) local variables are automatically in
  cache. Under this simplifying assumption, cache misses can only
  occur when accessing addresses not in the current stack frame.}. In
this paper, we allow for the program to control the eviction
strategy. This assumption is justified by the fact that standard
eviction strategies such as LRU, coupled with $\omega(1)$ resource
augmentation, are $(1 + o(1))$-competitive with the optimal eviction
strategy \cite{SleatorTa85}.

A natural approach to analyzing the cache behavior of a
\emph{parallel} algorithm (see, e.g., \cite{ParallelCacheAlg2}) is to
analyze the behavior of a \emph{serial execution} of the algorithm in
the External Memory Model (see, e.g., \cite{ParallelCacheAlg2}). In
this paper, we take a slightly more fine-grained approach to measuring
the cache performance. In particular, we explicitly model the
behavior of each processor's cache.

We assume that threads are scheduled using work stealing
\cite{AcarBl00, BlumofeLe99, FrigoLeRa98, AroraBlPl98, BlumofeJo96},
and that the work-stealing itself has no cache-miss overhead. We
further assume that \emph{each} processor has a cache of size
$M = \polylog n$ (for a polylog of our choice) that is analyzed using
the External Memory Model.

In order to keep our analysis of cache misses independent of the
number of processors, we will ignore the cost of warming up each
processor's cache (i.e., the first $M$ cache misses in each cache are
free).  All subsequent cache misses are counted, however, including
any cache misses necessary to warm up the cache after a
steal.\footnote{We continue to follow the convention that local
  variables, such as those passed to a thread via the arguments of a
  parallel for loop or function, do not incur cache misses, however.}
Finally, if a processor $p$ writes to a cache line, then the cache
line is automatically invalidated in (i.e., removed from) all other
processors' caches.

% Whereas the EREW memory model prohibits concurrent accesses to memory,
% on the other side of the spectrum are CRCW
% (concurrent-read-concurrent-write) models, which allow for both reads
% and writes to be performed concurrently (and in some variants even
% allow for atomic operations)
% \cite{Blelloch96,AcarBl16,MatiasVi95}. One approach to designing
% efficient EREW algorithms is to simulate efficient CRCW algorithms in
% the EREW model \cite{MatiasVi95}. The known simulation techniques
% require substantial space overhead, however, preventing the design of
% in-place algorithms \cite{MatiasVi95}.\footnote{The known simulation
%   techniques also increase the total work in the original algorithm,
%   although this can be acceptable if only a small number of atomic
%   operations need to be simulated.}

% In addition to being the first in-place and polylogarithmic-span EREW
% algorithms for the parallel-partition problem, our algorithms are also
% the first such CREW algorithms. In a \defn{CREW} algorithm, reads may
% be concurrent, but writes may not -- CREW stands for
% \emph{concurrent-read exclusive-write}. In practice, the important
% property of our algorithms is that they avoid concurrent writes (which
% can lead to non-determinacy and cache ping-pong effects).

%% HERE: Is where we can talk about the more powerful alternative to EREW, and there relationship, and why we can't just simulate CRCW (or, more importantly, ERCW). Make it clear that the Exclusive Read part is not actually very important -- it's the exclusive write part that's hard.

%% \footnote{The
  %% algorithm in this paper satisfies a slightly stronger property that
  %% the total memory being used is never more than $O(\log n) \cdot p$,
  %% where $p$ is an upper-bound on the number of worker threads.}

\paragraph{The Parallel Partition Problem}
The parallel partition problem takes an input array
$A = (A[1], A[2], \ldots, A[n])$ of size $n$, and a \defn{decider
  function} $\dec$ that determines for each element $A[i] \in A$
whether or not $A[i]$ is a \defn{predecessor} or a
\defn{successor}. That is, $\dec(A[i]) = 1$ if $A[i]$ is a
predecessor, and $\dec(A[i]) = 0$ if $A[i]$ is a successor. The
behavior of the parallel partition is to reorder the elements in the
array $A$ so that the predecessors appear before the successors. Note
that, in this paper, we will always treat arrays as 1-indexed.

\paragraph{The Sum-and-Swap Algorithm} The Sum-and-Swap Algorithm for
parallel partitioning consists of two phases \cite{Blelloch96,AcarBl16}:

\noindent\emph{The Sum Phase: }In this phase, the algorithm first
creates an array $D$ whose $i$-th element $D[i] = \dec(A[i])$. Then
the algorithm constructs an array $S$ whose $i$-th element
$S[i] = \sum_{j = 1}^i D[i]$ is the number of predecessors in the
first $i$ elements of $A$. The transformation from $D$ to $S$ is
called a \defn{parallel prefix sum} and can be performed with $O(n)$
work and $O(\log n)$ span using a simple recursive algorithm: (1)
First construct an array $D'$ of size $n / 2$ with
$D'[i] = D[2i - 1] + D[2i]$; (2) Recursively construct a parallel
prefix sum $S'$ of $D'$; (3) Build $S$ by setting each
$S[i] = S'[\lfloor i / 2 \rfloor] + A[i]$ for odd $i>1$ and
$S[i] = S'[i / 2]$ for even $i$, and $S[1] = A[1]$.

\noindent\emph{The Swap Phase: }In this phase, the algorithm constructs
an output-array $C$ by placing each predecessor $A[i] \in A$ in
position $S[i]$ of $C$. If there are $t$ predecessors in $A$, then the
first $t$ elements of $C$ will now contain those $t$ predecessors in
the same order that they appear in $A$. The algorithm then places each
successor $A[i] \in A$ in position $t + i - S[i]$. Since $i - S[i]$ is
the number of successors in the first $i$ elements of $A$, this places
the successors in $C$ in the same order that they appear in
$A$. Finally, the algorithm copies $C$ into $A$, completing the
parallel partition.

Both phases can be implemented with $O(n)$ work and $O(\log n)$
span. Like its serial out-of-place counterpart, the algorithm is
stable but not in place. The algorithm uses multiple auxiliary arrays
of size $n$ (one in each phase). Kiu, Knowles, and Davis
\cite{LiuKn05} were able to reduce the extra space consumption to
$n + p$ under the assumption that the number of processors $p$ is
hard-coded; their algorithm breaks the array $A$ into $p$ parts and
assigns one part to each thread. Reducing the extra space below $o(n)$
has remained open until now, even when the number of threads is fixed.

\section{A Cache-Efficient Parallel Partition}
\label{sec:recursiveSmoothedStriding}

In this section we present the \defn{Smoothed Striding
Algorithm}, which exhibits provably optimal cache behavior (up to
small-order factors). The Smoothed Striding Algorithm is fully
in-place and has polylogarithmic span. In particular, this means
that the total amount of auxiliary memory allocated at a given
moment in the execution never exceeds $\polylog n$ per active
worker. In this section we also analyze a version of the Smoothed
Striding Algorithm, called the Recursive Smoothed Striding
Algorithm, that is remarkably simple.

\paragraph{The Strided Algorithm \cite{FrancisPa92}}
The Smoothed Striding Algorithm borrows several structural ideas
from a previous algorithm of Francis and Pannan
\cite{FrancisPa92}, which we call the Strided Algorithm. The
Strided Algorithm is designed to behave well on random arrays
$A$, achieving span $\tilde{O}(n^{2/3})$ and exhibiting only $n/b
+ \tilde{O}(n^{2/3} / b)$  cache misses on such inputs. On
worst-case inputs, however, the Strided Algorithm has span
$\Omega(n)$ and incurs $n/b + \Omega(n/b)$ cache misses. Our
algorithm, the Smoothed Striding Algorithm, builds on the Strided
Algorithm by randomly perturbing the internal structure of the
original algorithm; in doing so, we are able to provide provable
performance guarantees for arbitrary inputs, and to add a
recursion step that was previously impossible. \\
The \defn{Strided Algorithm} consists of two steps:

\noindent\emph{The Partial Partition Step: }
Let $g \in \mathbb{N}$ be a parameter, and assume for simplicity
that $gb \mid n$. Logically partition the array $A$ into
$\frac{n}{gb}$ chunks $C_1, \ldots, C_{n / gb}$, each consisting
of $g$ cache lines of size $b$. For $i \in \{1, 2, \ldots, g\}$,
define $P_i$ to consist of the $i$-th cache line from each of the
chunks $C_1, \ldots, C_{n / gb}$. The $P_i$'s 
form a strided partition of array $A$, since consecutive cache
lines in $P_i$ are always separated by a fixed stride of $g - 1$
other cache lines.
The first step of the Strided Algorithm is to perform an in-place
serial partition on each of the $P_i$'s, rearranging the elements
within the $P_i$ so that the predecessors come first. This step
requires work $\Theta(n)$ and span $\Theta(n/g)$.

\noindent\emph{The Serial Cleanup Step: } For each $P_i$, define the
\defn{splitting position} $v_i$ to be the position in $A$ of the
first successor in (the already partitioned) $P_i$. Define
$v_{\text{min}} = \min\{v_1, \ldots, v_{g}\}$ and define
$v_{\text{max}} = \max\{v_1, \ldots, v_{g}\}$. 
The second step of the Strided Algorithm is to 
partition the sub-array ${A[v_{\text{min}}],\ldots,
A[v_{\text{max}}-1]}$ in serial. This step has no parallelism, and thus has
work and span $\Theta(v_{\text{max}} - v_{\text{min}})$.

In general, the lack of parallelism in the Serial Cleanup step 
results in an algorithm with linear-span (i.e., no
parallelism guarantee).  When the number of predecessors in each
of the $P_i$'s is close to equal, however, the quantity
$v_{\text{max}} - v_{\text{min}}$ can be much smaller than
$\Theta(n)$. For example, if $b = 1$, and if each element of $A$
is selected independently from some distribution, then one can
use Chernoff bounds to prove that with high probability in $n$,
$v_{\text{max}} - v_{\text{min}} \le O(\sqrt{n \cdot g \cdot \log
n})$.  The full span of the algorithm is then $\tilde{O}(n/g +
\sqrt{n \cdot g})$, which optimizes at $g = n^{1/3}$ to
$\tilde{O}(n^{2/3})$. Since the Partial Partition Step incurs
only $n / b$ cache misses, the full algorithm incurs $n +
\tilde{O}(n^{2/3})$ cache misses on a random array $A$.

Using Hoeffding's Inequality in place of Chernoff bounds, one can
obtain analogous bounds for larger values of $b$; in particular
for $b \in \polylog(n)$, the optimal span remains
$\tilde{O}(n^{2/3})$ and the number of cache misses becomes $n /
b + \tilde{O}(n^{2/3} / b)$ on an array $A$ consisting of
randomly sampled elements.\footnote{The original algorithm of
Francis and Pannan \cite{FrancisPa92} does not consider the
cache-line size $b$. Frias and Petit later introduced the
parameter $b$ \cite{Frias08}, and showed that by setting $b$
appropriately, one obtains an algorithm whose empirical
performance is close to the state-of-the-art.}

%% With this optimization, one advantage of the Strided Algorithm is that when $v_{\text{max}} - v_{\text{min}}$ is small, the total number of cache misses by the algorithm is close to the same as for a single scan through the data. 

\paragraph{The Smoothed Striding Algorithm}
To obtain an algorithm with provable guarantees for all inputs $A$, we
randomly perturb the internal structure of each of the $P_i$'s. Define
$U_1, \ldots, U_{g}$ (which play a role analogous to $P_1,
\ldots, P_g$ in the Strided Algorithm) so that each $U_i$ contains one
randomly selected cache line from each of $C_1, \ldots, C_{n /
gb}$ (rather than containing the $i$-th cache line of each
$C_j$). This ensures that the number of predecessors in each $U_i$ is
a sum of independent random variables with values in $\{0, 1, \ldots,
n/g\}$.

By Hoeffding's Inequality, with high probability in $n$, the number of
predecessors in each $U_i$ is tightly concentrated around $\frac{\mu
n}{g}$, where $\mu$ is the fraction of elements in $A$ that are
predecessors. It follows that, if we perform in-place partitions of
each $U_i$ in parallel, and then define $v_i$ to be the position in
$A$ of the first successor in (the already partitioned) $U_i$, then
the difference between $v_{\text{min}} = \min_i v_i$ and
$v_{\text{max}} = \max_i v_i$ will be small (regardless of the input array
$A$!).

Rather than partitioning $A[v_{\text{min}}],\ldots,
A[v_{\text{max}}-1]$ in serial, the Smoothed Striding Algorithm simply
recurses on the subarray. Such a recursion would not have been
productive for the original Strided Algorithm because the strided
partition $P_1', \ldots, P_g'$ used in the recursive subproblem would
satisfy $P_1' \subseteq P_1, \ldots, P_g' \subseteq P_g$ and thus each
$P_i'$ is already partitioned. That is, in the original Strided
Algorithm, the problem that we would recurse on is a worst-case input
for the algorithm in the sense that the partial partition step makes
no progress.

The main challenge in designing the Smoothed Striding Algorithm
becomes the construction of $U_1, \ldots, U_{g}$ without
violating the in-place nature of the algorithm. A natural approach
might be to store for each $U_i, C_j$ the index of the cache
line in $C_j$ that $U_i$ contains. This would require the storage of
$\Theta(n / b)$ numbers as metadata, however, preventing the algorithm
from being in-place. To save space, the key insight is to select a
random offset $X_j \in \{1, 2, \ldots, g\}$ within each $C_j$, and
then to assign the $(X_j + i \pmod g) + 1$-th cache line of $C_j$ to
$U_i$ for $i \in \{1, 2, \ldots, g\}$. This allows for us to construct
the $U_i$'s using only $O(n/(gb))$ machine words
storing the metadata $X_1, \ldots, X_{n / (gb)}$. By setting $g$ to
be relatively large, so that $n/(gb) \le \polylog(n)$, we can
obtain an in-place algorithm that incurs $(1 + o(1))n/b$ cache
misses.

We remark that the recursive structure of the Smoothed Striding
Algorithm allows for the algorithm to achieve polylogarithmic
span, and makes the algorithm very simple to implement in
practice. 

\paragraph{Formal Algorithm Description} Let $b < n$ be the size
of a cache line, let $A$ be an input array of size $n$, and let
$g$ be a parameter. (One should think of $g$ as being relatively
large, satisfying $n/(gb) \le \polylog(n)$.)  We assume for
simplicity that that $n$ is divisible by $gb$, and we define $s =
n/(gb)$. \footnote{This assumption can be made without loss of
  generality by treating $A$ as an array of size $n' = n + {(gb -
  n \pmod {gb})}$, and then treating the final $gb - n \pmod
  {gb}$ elements of the array as being successors (which
  consequently the algorithm needs not explicitly access). Note
  that the extra $n' - n$ elements are completely virtual,
  meaning they do not physically exist or reside in memory.
}

In the \defn{Partial Partition Step} the algorithm divides the
cache lines of $A$ into $g$ sets $U_1, \ldots, U_{g}$ where each
$U_i$ contains $s$ cache lines, and then performs a serial
partition on each $U_i$ in parallel over the $U_i$'s. To
determine the sets $U_1, \ldots, U_{g}$, the algorithm uses as
metadata an array $X = X[1], \ldots, X[s]$, where each $X[i] \in
\{1, \ldots, g\}$; in particular each of $X[1], \ldots, X[s]$ is
a uniformly random and independently selected element of $\{1, 2,
\ldots, g\}$. For each $i \in \{1, 2, \ldots, g\}$, $j \in \{1,
2, \ldots, s\}$, define $$G_i(j) = (X[j] + i \pmod g) + (j - 1)g
+ 1.$$ Using this terminology, we define each $U_i$ for $i \in
\{1, \ldots, g\}$ to contain the $G_i(j)$-th cache line of $A$
for each $j \in \{1, 2, \ldots, s\}$. That is, $G_i(j)$ denotes
the index of the $j$-th cache line from array $A$ contained in
$U_i$.

Note that, to compute the index of the $j$-th cache line in $U_i$,
one needs only the value of $X[j]$. Thus the only metadata needed by
the algorithm to determine $U_1, \ldots, U_g$ is the array
$X$. If $|X| = s = \frac{n}{gb} \le \polylog(n)$, then
the algorithm is in place.
  
The algorithm performs an in-place (serial) partition on each
$U_i$ (and performs these partitions in parallel with one
another). In doing so, the algorithm, also collects
$v_{\text{min}}=\min_i{v_i}$, $v_{\text{max}}=\max_i{v_i}$, where
each $v_i$ with $i \in \{1, \ldots, g\}$ is defined to be the index
of the first successor in $A$ (or $n$ if no such successor
exists).\footnote{One can calculate $v_{\text{min}}$ and
  $v_{\text{max}}$ without explicitly storing each of $v_1, \ldots,
  v_{g}$ as follows. Rather than using a standard $g$-way parallel
  for-loop to partition each of $U_1, \ldots, U_{g}$, one can
  manually implement the parallel for-loop using a recursive
  divide-and-conquer approach. Each recursive call in the
  divide-and-conquer can then simply collect the maximum and minimum
  $v_i$ for the $U_i$'s that are partitioned within that recursive
  call. This adds $O(\log n)$ to the total span of the Partial
  Partition Step, which does not affect the overall span
  asymptotically.
}

The array $A$ is now ``partially partitioned", i.e. $A[i]$ is a
predecessor for all $i \le v_{\text{min}}$, and $A[i]$ is a successor
for all $i > v_{\text{max}}$.

The second step of the Smoothed Striding Algorithm is to complete
the partitioning of $A[v_{\text{min}} + 1], \ldots,
A[v_{\text{max}}]$. The \defn{Recursive Smoothed Striding
Algorithm} partitions $A[v_{\text{min}} + 1], \ldots,
A[v_{\text{max}}]$ recursively using the same algorithm (and
resorts to a serial base case when the subproblem is small enough
that $g \le O(1)$). In Section~\ref{sec:hybridSmoothedStriding}
we discuss a different variant of the Smoothed Striding
Algorithm, called the \defn{Hybrid Smoothed Striding Algorithm},
which partitions $A[v_{\text{min}} + 1], \ldots,
A[v_{\text{max}}]$ using the in-place algorithm given in
Theorem~\ref{thminplace} instead of recursing. In general, the
Hybrid algorithm yields better theoretical guarantees on span
than the recursive version; on the other hand, the recursive
version has the advantage that it is simple to implement as fully
in-place, and still achieves polylogarithmic span. Detailed
pseudocode for the Recursive Smoothed Striding Algorithm can be
found in Appendix \ref{sec:pseudocode}. 

%% note: g > s should hold
\paragraph{Algorithm Analysis} 
Our first proposition analyzes the Partial Partition Step of the
Smoothed Striding Algorithm.
\begin{proposition}
  \label{prop:generalResult}
  %% The 1/2's are necessary for the final line of the proof to easily go through.
  
  Let $\epsilon \in (0, 1/2)$ and $\delta \in (0, 1/2)$ such that
  $\epsilon \ge 1/\poly(n)$ and $\delta \ge 1/\polylog(n)$.
  Suppose $s > \frac{\ln (n/\epsilon)}{\delta^2}$, and that each
  processor has a cache of size at least $s + c$ for a
  sufficiently large constant $c$.

  Then the Partial-Partition Step achieves work $O(n)$; achieves
  span $O(b \cdot s)$; incurs $\frac{s+n}{b} + O(1)$ cache
  misses; and guarantees that with probability at least $1 - \epsilon$,
  $$v_{\text{max}}-v_{\text{min}} < 4 n \delta.$$
\end{proposition}
\begin{proof}
Since $\sum_i |U_i| = n$, and since the serial partitioning of each $U_i$
takes time $O(|U_i|)$, the total work performed by the algorithm is
$O(n)$.

To analyze cache misses, we assume without loss of generality that
array $X$ is pinned in each processor's cache (note, in particular,
that $|X| = s \le \polylog(n)$, and so $X$ fits in cache), and that
$X$ is loaded into each processor's cache as the cache is warmed
up. Thus we can ignore the cost of accesses to $X$, and treat all
other accesses as being in a $\polylog n$ cache managed using
LRU.

Note that each $U_i$ consists of $s = \polylog n$ cache lines,
meaning that each $U_i$ fits entirely in cache. Thus the number of
cache misses needed for a thread to partition a given $U_i$ is just
$s$. Since there are $g$ of the $U_i's$, the total number of cache
misses incurred in partitioning all of the $U_i$'s is $g s =
n/b$. Besides these, there are $s/b$ cache misses for instantiating
the array $X$; and $O(1)$ cache misses for other instantiating
costs. This sums to $$\frac{n+s}{b}+O(1).$$

The span of the algorithm is $O(n/g + s) = O(b\cdot s)$, since the
each $U_i$ is of size $O(n / g)$, and because the initialization of
array $X$ can be performed in time $O(|X|) = O(s)$.

It remains to show that with probability $1-\epsilon$, $v_{\text{max}}
- v_{\text{min}} < 4n\delta$. Let $\mu$ denote the fraction of
elements in $A$ that are predecessors. For $i \in \{1, 2, \ldots,
g\}$, let $\mu_i$ denote the fraction of elements in $U_i$ that are
predecessors. Note that each $\mu_i$ is the average of $s$ independent
random variables $Y_i(1), \ldots, Y_i(s) \in [0, 1]$, where $Y_i(j)$
is the fraction of elements in the $G_i(j)$-th cache line of $A$ that
are predecessors. By construction, $G_i(j)$ has the same probability
distribution for all $i$, since $(X[j] + i) \pmod g$ is uniformly
random in $\mathbb{Z}_g$ for all $i$. It follows that $Y_i(j)$ has the
same distribution for all $i$, and thus that $\E[\mu_i]$ is
independent of $i$. Since the average of the $\mu_i$s is $\mu$, it
follows that $\E[\mu_i] = \mu$ for all $i \in \{1, 2, \ldots, g\}$.

Since each $\mu_i$ is the average of $s$ independent $[0, 1]$-random
variables, we can apply Hoeffding's inequality (i.e. a Chernoff Bound
for a random variable on $[0,1]$ rather than on $\{0,1\}$) to each
$\mu_i$ to show that it is tightly concentrated around its expected
value $\mu$, i.e.,
$$\Pr[|\mu_i - \mu| \geq \delta] < 2\exp(-2s\delta^2). $$

Since $s > \frac{\ln (n/\epsilon)}{\delta^2} \ge \frac{\ln (2n /
  (b\epsilon))}{2\delta^2}$, we find that for all $i \in
  \{1,\ldots, g\}$, $$\Pr[|\mu_i - \mu| \geq \delta] <
  2\exp\Big({-2} \frac{\ln (2n/(b\epsilon))}{2\delta^2}
  \delta^2\Big) = \frac{\epsilon}{n/b} < \frac{\epsilon}{g}. $$
  By the union bound, it follows that with probability at least
  $1 - \epsilon$, all of $\mu_1, \ldots, \mu_{g}$ are within
  $\delta$ of $\mu$.

%% We use this bound on the probability of any individual group $U_y$ failing to meet the condition $|\mu-\mu_y| < \delta$ to bound the probability that at least one of the groups $U_0, \ldots, U_{g-1}$ fails to meet the condition.
%% Note that the probability of at least one group failing is: 
%% $$\Pr\Big[\bigvee_{y=0}^{g-1} |\mu_y - \mu| \geq \delta\Big].$$
%% This is bounded by 
%% $$\Pr\Big[\bigvee_{y=0}^{g-1} |\mu_y - \mu| \geq \delta\Big] \leq \sum_{y=0}^{g-1} \Pr[|\mu_y - \mu| \geq \delta] < \epsilon.$$
%% Thus the event occurs with probability bounded above by $\epsilon$, the specified failure probability.

To complete the proof we will show that the occurrence of the event
that all $\mu_y$ simultaneously satisfy $|\mu - \mu_y| < \delta$ implies
that $v_{\text{max}} - v_{\text{min}} \le 4n\delta$.

%% Let $U_i(j)$ denote the index in $A$ of the $j$-th element in
%% $U_i$.satisfies
%% $$gj - bg + 1 \le U_i(j) \le gj + bg$$ for all $j$.

%% It follows
%% that $v_i = U_i(\mu_i \cdot |U_i|)$ is within $bg$ of $g
%% \mu_j \cdot |U_i| = \mu_j n$. Therefore,
%% $$|v_i - \mu n| \le bg + (\mu_j - \mu) n < bg + \delta n.$$ This
%% implies that the maximum of $|v_i - v_j|$ for
%% any $i$ and $j$ is at most, $2bg + 2\delta n$. Thus,
%% \begin{align*}
%%   v_{\text{max}} - v_{\text{min}} & \le 2n \left( \delta + \frac{n}{bg} \right)  = 2n \left( \delta + s \right) \\
%%   & \le 2n \left(\delta + \frac{2\delta^2}{\ln (2n / (b\epsilon))}\right) < 4n\cdot\delta.
%% \end{align*}

  Recall that $G_i(j)$ denotes the index within $A$ of the $j$ th
  cache-line contained in $U_i$. By the definition of $G_i(j)$,
  $$(j - 1)g + 1 \le G_i(j) \le jg.$$ Note that $A[v_i]$ will
  occur in the $\lceil s\mu_i \rceil$-th cache-line of $U_i$
  because $U_i$ is composed of $s$ cache lines. Hence $$(\lceil
  s\mu_i \rceil - 1) g b + 1 \le v_i \le \lceil s\mu_i \rceil g
  b,$$ which means that
  $$s\mu_i g b - gb + 1 \le v_i \le s\mu_i g b + gb.$$ Since $sgb = n$, 
  it follows that $|v_i - n \mu_i| \le gb$. Therefore,
  $$|v_i - n \mu| < gb + n\delta.$$
  This implies that the maximum of $|v_i - v_j|$ for
any $i$ and $j$ is at most, $2bg + 2\delta n$. Thus,

$$v_{\text{max}} - v_{\text{min}} \le 2n \left( \delta +
\frac{bg}{n} \right)  = 2n \left( \delta + 1/s \right) \le
2n \left(\delta + \frac{\delta^2}{\ln (n / \epsilon)}\right) <
4n\cdot\delta.$$

% \begin{align*}
%   v_{\text{max}} - v_{\text{min}} & \le 2n \left( \delta + \frac{bg}{n} \right)  = 2n \left( \delta + 1/s \right) \\
%   & \le 2n \left(\delta + \frac{\delta^2}{\ln (n / \epsilon)}\right) < 4n\cdot\delta.
% \end{align*}
\end{proof}

We use Proposition~\ref{prop:generalResult} as a tool to analyze
the Recursive Smoothed Striding Algorithm.
Rather than parameterizing the Partial Partition step in each
algorithm by $s$, Proposition~\ref{prop:generalResult} suggests
that it is more natural to parameterize by $\epsilon$ and
$\delta$, which then determine $s$.

We choose $\epsilon = 1/n^c$ for $c$ of our choice (i.e. we want
to guarantee success with high probability in $n$). Moreover, the
Recursive Smoothed Striding Algorithm continues to use the same
value of $\epsilon$ within recursive subproblems (i.e., the
$\epsilon$ is chosen based on the size of the first subproblem in
the recursion), so that the entire algorithm succeeds with high
probability in $n$.

The choice of $\delta$ results in a trade-off between cache
misses and span. For the Recursive Smoothed Striding Algorithm we
allow $\delta$ to be chosen arbitrarily at the top level of
recursion, and then fix $\delta  = \Theta(1)$ to be a
sufficiently small constant at all levels of recursion after the
first; this guarantees that we at least halve the size of the
problem between recursive iterations\footnote{In general, setting
  $\delta = 1/8$ will result in the problem size being halved.
  However, this relies on the assumption that $gb \mid n$, which
  is only without loss of generality by allowing for the size of
subproblems to be sometimes artificially increased by a small
amount (i.e., a factor of $1 + gb / n = 1 + 1/s$). One can handle
this issue by decreasing $\delta$ to, say, $1/16$.}. Optimizing
$\delta$ further (after the first level of recursion) would only
affect the number of undesired cache misses by a constant factor.

Now we analyze the Recursive Smoothed Striding Algorithm. We
prove the following theorem:
\begin{theorem}
  \label{thm:groupedPartitionAlg}
  With high probability in $n$, the Recursive Smoothed Striding
  algorithm using parameter $\delta \in(0,1/2)$ satisfying
  $\delta \ge 1 / \polylog(n)$: achieves work $O(n)$, attains
  span $$O\left(b\left(\log^2 n + \frac{\log
  n}{\delta^2}\right)\right),$$ and incurs $(n+O(n \delta))/b$
  cache misses. 
\end{theorem}

A particularly natural parameter setting for the Recursive
Smoothed Striding algorithm occurs at $\delta = 1 / \sqrt{\log n}$.
\begin{corollary}[Corollary of Theorem \ref{thm:groupedPartitionAlg}]
  \label{cor:groupedPartitionAlg} With high probability in $n$,
  the Recursive Smoothed Striding Algorithm using parameter
  $\delta=1/\sqrt{\log n}$: achieves work $O(n)$, attains span
  $O(b\log^2 n)$, and incurs $(1 + o(1))n/b$ cache misses. 
\end{corollary}

\begin{proof}[Proof of Theorem \ref{thm:groupedPartitionAlg}]
  To avoid confusion, we use $\delta'$, rather than $\delta$, to
  denote the constant value of $\delta$ used at levels of recursion
  after the first.
  
  
  By Proposition \ref{prop:generalResult}, the top level of the algorithm
  has work $O(n)$, span $O\left(b\frac{\log n}{\delta^2}\right),$ and
  incurs $\frac{s+n}{b} + O(1)$ cache misses.  The recursion reduces
  the problem size by at least a factor of $4\delta$, with high
  probability in $n$.

  At lower layers of recursion, with high probability in $n$, the
  algorithm reduces the problem size by a factor of at least
  $1/2$ (since $\delta$ is set to be a sufficiently small
  constant). For each $i > 1$, it follows that the size of the
  problem at the $i$-th level of recursion is at most $O(n \delta
  / 2^i)$.
  
  The sum of the sizes of the problems after the first level of
  recursion is therefore bounded above by a geometric series summing to at most $O(n
  \delta)$. This means that the total work of the algorithm is at most
  $O(n\delta) + O(n) \le O(n)$.

  Recall that each level $i > 1$ uses $s =
  \frac{\ln(2^{-i}n\delta'/b)}{\delta'^2}$, where $\delta' =
  \Theta(1)$. It follows that level $i$ uses $s \le O(\log n)$.
  Thus, by Proposition \ref{prop:generalResult}, level $i$
  contributes $O(b\cdot s)=O(b \log n)$ to the span.  Since there
  are at most  $O(\log n)$ levels of recursion, the total span in
  the lower levels of recursion is at most $O(b\log^2 n)$, and
  the total span for the algorithm is at most,
  $$O\left(b\left(\log^2 n + \frac{\log
  n}{\delta^2}\right)\right).$$
        
  To compute the total number of cache misses of the algorithm,
  we add together $(n+s)/b+O(1)$ for the top level, and then, by
  Proposition \ref{prop:generalResult}, at most $$\sum_{0 \leq i<
    O(\log n)}\frac{1}{b} O\left(2^{2-i}n\delta + \log n\right) \le
  O\left(\frac{1}{b} (n \delta + \log^2 n)\right).$$ for lower
  levels. Thus the total number of cache misses for the algorithm
  is, $$\frac{1}{b}\left(n+\frac{\log n}{\delta^2 }\right) +
  O(n\delta + \log^2 n) / b = (n+O(n\delta))/b.$$ 
\end{proof}

\begin{proof}[Proof of Corollary \ref{cor:groupedPartitionAlg}]
  By Theorem \ref{thm:groupedPartitionAlg}, with high probability
  in $n$, the algorithm has work $O(n)$, the algorithm has span
  $$O\left(b\left(\log^2 n + \frac{\log
  n}{\delta^2}\right)\right) = O(b\log^2 n),$$ and the algorithm
  incurs $$(n+O(n\delta))/b = (n+O(n/\sqrt{\log n}))/b =
  (n+o(n))/b$$ cache misses.
\end{proof}

\section{An $O(\log n \log \log n)$-Span Parallel Partition}
\label{sec:blockedprefixsumpartitionalg}

In this section, we present an in-place algorithm, called the
\defn{In-Place Sum-and-Swap Algorithm}, for parallel
partition with span $O(\log n \log \log n)$. Each thread in the
algorithm requires memory at most $O(\log n)$. 

In addition to being theoretically interesting on its own, the
In-Place Sum-and-Swap Algorithm is useful as a subroutine in the
Smoothed Striding Algorithm. The In-Place Sum-and-Swap Algorithm
has poor cache behavior, and so performs somewhat poorly in
practice. However, using it as a subroutine on a small subproblem
in the Smoothed Striding Algorithm allows the Smoothed Striding
Algorithm to achieve span $O(\log n \log\log n)$ without
sacrificing its cache behavior; we discuss this in detail in
Section~\ref{sec:hybridSmoothedStriding}. We now describe the
In-Place Sum-and-Swap Algorithm. 

Prior to beginning the algorithm, the first implicit step of the
algorithm is to count the number of predecessors in the array, in
order to determine whether the majority of elements are either
predecessors or successors. Throughout the rest of the section, we
assume without loss of generality that the total number of successors
in $A$ exceeds the number of predecessors, since otherwise their roles
can simply be swapped in the algorithm. Further, we assume for
simplicity that the elements of $A$ are distinct; this assumption is
removed at the end of the section.

\paragraph{Algorithm Outline}
We begin by presenting an overview of the key algorithmic ideas needed
to construct an in-place algorithm.

Consider how to remove the auxiliary array $C$ from the Reordering
Phase. If one attempts to simply swap in parallel each predecessor
$A[i]$ with the element in position $j = S[i]$ of $A$, then the swaps
will almost certainly conflict. Indeed, $A[j]$ may also be a
predecessor that needs to be swapped with $A[S[j]]$. Continuing like
this, there may be an arbitrarily long list of dependencies on the
swaps.

To combat this, we begin the algorithm with a Preprocessing Phase in
which $A$ is rearranged so that every prefix is
\defn{successor-heavy}, meaning that for all $t$, the first $t$
elements contain at least $\frac{t}{4}$ successors. Then we compute
the prefix-sum array $S$, and begin the Reordering Phase. Using the
fact that the prefixes of $A$ are successor-heavy, the reordering can
now be performed in place as follows: (1) We begin by recursively
reordering the prefix $P$ of $A$ consisting of the first $4/5 \cdot n$
elements, so that the predecessors appear before the successors; (2)
Then we simply swap each predecessor $A[i]$ in the final $1/5 \cdot n$
elements with the corresponding element $S[A[i]]$. The fact that the
prefix $P$ is successor-heavy ensures that, after step (1), the final 
$\frac{1}{5} \cdot n$ elements of (the reordered) $P$ are successors. 
This implies in step (2) that for each of the swaps between predecessors $A[i]$
in the final $1/5 \cdot n$ elements and earlier positions $S[A[i]]$, the latter
element will be in the prefix $P$. In other words, the swaps are now conflict
free.

Next consider how to remove the array $S$ from the Parallel-Prefix
Phase. At face value, this would seem quite difficult since the
reordering phase relies heavily on $S$. Our solution is to
\emph{implicitly} store the value of every $O(\log n)$-th element of
$S$ in the ordering of the elements of $A$. That is, we break $A$ into
blocks of size $O(\log n)$, and use the order of the elements in each
block to encode an entry of $S$. (If the elements are not all
  distinct, then a slightly more sophisticated encoding is necessary.)
Moreover, we modify the algorithm for building $S$ to only construct
every $O(\log n)$-th element. The new parallel-prefix sum performs
$O(n / \log n)$ arithmetic operations on values that are implicitly
encoded in blocks; since each such operation requires $O(\log n)$
work, the total work remains linear.

In the remainder of the section, we present the algorithm in detail,
and prove the key properties of each phase of the algorithm. We also
provide detailed pseudocode in Appendix \ref{sec:pseudocode}. The
algorithm proceeds in three phases.

\paragraph{A Preprocessing Phase}
The goal of the Preprocessing phase is to make every prefix of $A$
successor-heavy. To perform the Preprocessing phase on $A$, we begin
with a parallel-for-loop: For each $i = 1, \ldots, \lfloor n /
2\rfloor$, if $A[i]$ is a predecessor and $A[n - i + 1]$ is a
successor, then we swap their positions in $A$. To complete the
Preprocessing phase on $A$, we then recursively perform a
Preprocessing phase on $A[1], \ldots, A[\lceil n / 2 \rceil]$.

\begin{lemma}
 The Preprocessing Phase has work $O(n)$ and span $O(\log n)$. At the
 end of the Preprocessing Phase, every prefix of $A$ is
 successor-heavy.
  \label{lem:preprocessingphase}
\end{lemma}
\begin{proof}
Recall that for each $t \in 1, \ldots, n$, we call the $t$-prefix
$A[1], \ldots, A[t]$ of $A$ successor-heavy if it contains at least
$\frac{t}{4}$ successors.

The first parallel-for-loop ensures that at least half the successors
in $A$ reside in the first $\lceil n / 2 \rceil$ positions, since for
$i = 1, \ldots, \lfloor n / 2 \rfloor$, $A[n - i + 1]$ will only be a
successor if $A[i]$ is also a successor. Because at least half the
elements in $A$ are successors, it follows that the first $\lceil n /
2 \rceil$ positions contain at least $\lceil n / 4\rceil$ successors,
making every $t$-prefix with $t \ge \lceil n / 2 \rceil$
successor-heavy.

After the parallel-for-loop, the first $\lceil n / 2 \rceil$ positions
of $A$ contain at least as many successors as predecessors (since
$\lceil n / 4 \rceil \ge \frac{\lceil n / 2 \rceil}{2}$). Thus we can
recursively apply the argument above in order to conclude that the
recursion on $A[1], \ldots, A[\lceil n / 2 \rceil]$ makes every
$t$-prefix with $t \le \lceil n / 2 \rceil$ successor-heavy. It
follows that, after the recursion, every $t$-prefix of $A$ is
successor-heavy.

Each recursive level has constant span and performs work proportional
to the size of the subarray being considered. The Preprocessing phase
therefore has total work $O(n)$ and span $O(\log n)$.
\end{proof}


%% Recall that for each $t \in 1, \ldots, n$, we call the $t$-prefix
%% $A[1], \ldots, A[t]$ of $A$ successor-heavy if it contains at least
%% $\frac{t}{4}$ successors. The goal of the preprocessing phase is to
%% rearrange $A$ so that every prefix is successor heavy.

through each of the blocks $X_{t + 1}, \ldots, X_{\lfloor n / b \rfloor}$. For each block
$X_i$, we first extract $v_i$ (with work $O(\log n)$ and span $O(\log
\log n)$ using Lemma \ref{lem:bitstore}). We then create an auxiliary
array $Y_i$ of size $|X_i|$, using $O(\log n)$ thread-local
memory. Using a parallel-prefix sum (with work $O(\log n)$ and span
$O(\log \log n)$), we set each $Y_i[j]$ equal to $v_i$ plus the number
of predecessors in $X_i[1], \ldots, X_i[j]$. In other words, $Y_i[j]$
equals the number of predecessors in $A$ appearing at or before
$X_i[j]$.

After creating $Y_i$, we then perform a parallel-for-loop through the
elements $X_i[j]$ of $X_i$ (note we are still within another parallel
loop through the $X_i$'s), and for each predecessor $X_i[j]$, we swap
it with the element in position $Y_i[j]$ of the array $A$. This
completes the algorithm.

\begin{lemma}
 The Reordering phase takes work $O(n)$ and span $O(\log n \log \log
 n)$. At the end of the phase, the array $A$ is fully partitioned.
\end{lemma}
\begin{proof}
  After $P$ has been recursively partitioned, it will be of the form
  $P_1 \circ P_2$ where $P_1$ contains only predecessors and $P_2$
  contains only successors. Because $P$ was successor-heavy before the
  recursive partitioning (by Lemma \ref{lem:parallelprefix}), we have
  that $|P_2| \ge |P| / 4$, and thus that
  $|P_2| \ge |X_{t + 1} \circ \cdots \circ X_{\lfloor n / b
    \rfloor}|$.

After the recursion, the swaps performed by the algorithm will swap
the $i$-th predecessor in $X_{t + 1} \circ \cdots \circ X_{\lfloor n /
  b \rfloor}$ with the $i$-th element in $P_2$, for $i$ from $1$ to
the number of predecessors in $X_{t + 1} \circ \cdots \circ X_{\lfloor
  n / b \rfloor}$. Because $|P_2| \ge |X_{t + 1} \circ \cdots \circ
X_{\lfloor n / b \rfloor}|$ these swaps are guaranteed not to conflict
with one-another; and since $P_2$ consists of successors, the final
state of array $A$ will be fully partitioned.

The total work in the reordering phase is $O(n)$ since each $X_i$
appears in a parallel-for-loop at exactly one level of the recursion,
and incurs $O(\log n)$ work. The total span of the reordering phase is
$O(\log n \cdot \log \log n)$, since there are $O(\log n)$ levels of
recursion, and within each level of recursion each $X_i$ in the
parallel-for-loop incurs span $O(\log \log n)$. 
\end{proof}

Combining the phases, the full algorithm has work $O(n)$ and span
$O(\log \log n)$. Thus we have:
\begin{theorem}
  There exists an in-place algorithm using exclusive-read-write
  variables that performs parallel-partition with work $O(n)$ and span
  $O(\log n \cdot \log \log n)$.
  \label{thminplace}
\end{theorem}
%% We begin with a parallel-for-loop: For each $i = 1, \ldots, \lfloor n
%% / 2\rfloor$, if $A[i]$ is a predecessor and $A[n - i + 1]$ is a
%% successor, then we swap their positions in $A$.

%% This ensures that at least half the successors in $A$ reside in the
%% first $\lceil n / 2 \rceil$ positions. Thus the first $\lceil n / 2
%% \rceil$ positions contain at least $\lceil n / 4\rceil$ successors,
%% making every $t$-prefix with $t \ge \lceil n / 2 \rceil$
%% successor-heavy.

%% Since $\lceil n / 4 \rceil \ge \frac{\lceil n / 2 \rceil}{2}$, the
%% first $\lceil n / 2 \rceil$ positions of $A$ now contain at least as
%% many successors as predecessors. Thus we can recursively repeat the
%% same process on the subarray $A[1], \ldots, A[\lceil n / 2 \rceil]$
%% in order to make each of its prefixes successor-heavy.

%% Each recursive step has constant span and performs work proportional
%% to the size of the subarray being considered. The preprocessing phase
%% therefore has total work $O(n)$ and span $O(\log n)$.

\paragraph{An Implicit Parallel Prefix Sum}
Pick a \defn{block-size} $b \in \Theta(\log n)$ satisfying $b \ge 2
\lceil \log (n + 1) \rceil$. Consider $A$ as a series of $\lfloor n /
b \rfloor$ blocks of size $b$, with the final block of size between
$b$ and $2b - 1$. Denote the blocks by $X_1, \ldots, X_{\lfloor n / b
  \rfloor}$.

Within each block $X_i$, we can implicitly store a value in the range
$0, \ldots, n$ through the ordering of the elements:
\begin{lemma}
Given an array $X$ of $2 \lceil \log (n + 1) \rceil$ distinct
elements, and a value $v \in \{0, \ldots, n\}$, one can rearrange the
elements of $X$ to encode the bits of $v$ using work $O(\log n)$ and
span $O(\log \log n)$; and one can then later decode $v$ from $X$
using work $O(\log n)$ and span $O(\log \log n)$.
\label{lem:bitstore}
\end{lemma}
\begin{proof}
Observe that $X$ can be broken into (at least) $\lceil \log (n + 1)
\rceil$ disjoint pairs of adjacent elements $(x_1, x_2), (x_3, x_4),
\ldots$, and by rearranging the order in which a given pair $(x_j,
x_{j + 1})$ occurs, the lexicographic comparison of whether $x_j <
x_{j + 1}$ can be used to encode one bit of information. Values $v \in
[0,n]$ can therefore be read and written to $X$ with work $O(b) =
O(\log n)$ and span $O(\log b) = O(\log \log n)$ using a simple
divide-and-conquer recursive approach to encode and decode the bits of
$v$.
\end{proof}

To perform the Parallel Prefix Sum phase, our algorithm begins by
performing a parallel-for loop through the blocks, and storing in each
block $X_i$ a value $v_i$ equal to the number of predecessors in the
block. (This can be done in place with work $O(n)$ and span $O(\log
\log n)$ by Lemma \ref{lem:bitstore}.)

The algorithm then performs an in-place parallel-prefix operation on
the values $v_1, \ldots, v_{\lfloor n / b \rfloor}$ stored in the
blocks. This is done by first resetting each even-indexed value
$v_{2i}$ to $v_{2i} + v_{2i - 1}$; then recursively performing a
parallel-prefix sum on the even-indexed values; and then replacing
each odd-indexed $v_{2i + 1}$ with $v_{2i + 1} + v_{2i}$, where $v_0$
is defined to be zero.

Lemma \ref{lem:parallelprefix} analyzes the phase:
\begin{lemma}
  The Parallel Prefix Sum phase uses work $O(n)$ and span
  $O(\log n \log \log n)$. At the end of the phase, each $X_i$ encodes
  a value $v_i$ counting the number of predecessors in the prefix
  $X_1 \circ X_2 \circ \cdots \circ X_i$; and each prefix of blocks
  (i.e., each prefix of the form
  $X_1 \circ X_2 \circ \cdots \circ X_i$) is successor-heavy.
\label{lem:parallelprefix}
\end{lemma}
\begin{proof}
If the $v_i$'s could be read and written in constant time, then the
prefix sum would take work $O(n / \log n)$ and span $O(\log n)$, since
there are $O(n / \log n)$ $v_i$'s. Because each $v_i$ actually
requires work $O(\log n)$ and span $O(\log \log n)$ to read/write (by
Lemma \ref{lem:bitstore}), the prefix sum takes work $O(n)$ and span
$O(\log n \cdot \log \log n)$.

Once the prefix-sum has been performed, every block $X_i$
encodes a value $v_i$ counting the number of predecessors in the
prefix $X_1 \circ X_2 \circ \cdots \circ X_i$. Moreover, because the
Parallel Prefix Sum phase only rearranges elements within each $X_i$,
Lemma \ref{lem:preprocessingphase} ensures that each prefix of the
form $X_1 \circ X_2 \circ \cdots \circ X_i$ remains successor-heavy.
\end{proof}

\paragraph{In-Place Reordering}
In the final phase of the algorithm, we reorder $A$ so that the
predecessors appear before the successors. Let $P = X_1 \circ X_2
\circ \cdots \circ X_t$ be the smallest prefix of blocks that contains
at least $4/5$ of the elements in $A$. We begin by recursively
reordering the elements in $P$ so that the predecessors appear before
the successors; as a base case, when $|P| \le 5b = O(\log n)$, we
simply perform the reordering in serial.

To complete the reordering of $A$, we perform a parallel-for-loop
through each of the blocks $X_{t + 1}, \ldots, X_{\lfloor n / b \rfloor}$. For each block
$X_i$, we first extract $v_i$ (with work $O(\log n)$ and span $O(\log
\log n)$ using Lemma \ref{lem:bitstore}). We then create an auxiliary
array $Y_i$ of size $|X_i|$, using $O(\log n)$ thread-local
memory. Using a parallel-prefix sum (with work $O(\log n)$ and span
$O(\log \log n)$), we set each $Y_i[j]$ equal to $v_i$ plus the number
of predecessors in $X_i[1], \ldots, X_i[j]$. In other words, $Y_i[j]$
equals the number of predecessors in $A$ appearing at or before
$X_i[j]$.

After creating $Y_i$, we then perform a parallel-for-loop through the
elements $X_i[j]$ of $X_i$ (note we are still within another parallel
loop through the $X_i$'s), and for each predecessor $X_i[j]$, we swap
it with the element in position $Y_i[j]$ of the array $A$. This
completes the algorithm.

\begin{lemma}
 The Reordering phase takes work $O(n)$ and span $O(\log n \log \log
 n)$. At the end of the phase, the array $A$ is fully partitioned.
\end{lemma}
\begin{proof}
  After $P$ has been recursively partitioned, it will be of the form
  $P_1 \circ P_2$ where $P_1$ contains only predecessors and $P_2$
  contains only successors. Because $P$ was successor-heavy before the
  recursive partitioning (by Lemma \ref{lem:parallelprefix}), we have
  that $|P_2| \ge |P| / 4$, and thus that
  $|P_2| \ge |X_{t + 1} \circ \cdots \circ X_{\lfloor n / b
    \rfloor}|$.

After the recursion, the swaps performed by the algorithm will swap
the $i$-th predecessor in $X_{t + 1} \circ \cdots \circ X_{\lfloor n /
  b \rfloor}$ with the $i$-th element in $P_2$, for $i$ from $1$ to
the number of predecessors in $X_{t + 1} \circ \cdots \circ X_{\lfloor
  n / b \rfloor}$. Because $|P_2| \ge |X_{t + 1} \circ \cdots \circ
X_{\lfloor n / b \rfloor}|$ these swaps are guaranteed not to conflict
with one-another; and since $P_2$ consists of successors, the final
state of array $A$ will be fully partitioned.

The total work in the reordering phase is $O(n)$ since each $X_i$
appears in a parallel-for-loop at exactly one level of the recursion,
and incurs $O(\log n)$ work. The total span of the reordering phase is
$O(\log n \cdot \log \log n)$, since there are $O(\log n)$ levels of
recursion, and within each level of recursion each $X_i$ in the
parallel-for-loop incurs span $O(\log \log n)$. 
\end{proof}

Combining the phases, the full algorithm has work $O(n)$ and span
$O(\log \log n)$. Thus we have:
\begin{theorem}
  \label{thminplace}
  There exists an in-place algorithm using exclusive-read-write
  variables that performs parallel-partition with work $O(n)$ and span
  $O(\log n \cdot \log \log n)$.
\end{theorem}

\paragraph{Allowing for Repeated Elements}
In proving Theorem \ref{thminplace} we assumed for simplicity that the
elements of $A$ are distinct. To remove this assumption, we conclude
the section by proving a slightly more complex variant of Lemma
\ref{lem:bitstore}, eliminating the requirement that the elements of
the array $X$ be distinct:

\begin{lemma}
Let $X$ be an array of $b = 4 \lceil \log (n + 1) \rceil + 2$
elements. The there is an \emph{encode} function, and a \emph{decode}
function such that:
\begin{itemize}
\item The encode function modifies the array $X$ (possibly overwriting
  elements in addition to rearranging them) to store a value $v \in
  \{0, \ldots, n\}$. The first time the encode function is called on
  $X$ it has work and span $O(\log n)$. Any later times the encode
  function is called on $X$, it has work $O(\log n)$ and span $O(\log
  \log n)$. In addition to being given an argument $v$, the encode
  function is given a boolean argument indicating whether the function
  has been invoked on $X$ before.
\item The decode function recovers the value of $v$ from the modified
  array $X$, and restores $X$ to again be an array consisting of the
  same multiset of elements that it began with. The decode function
  has work $O(\log n)$ and span $O(\log \log n)$.
\end{itemize}
  \label{lem:bitstore2}
\end{lemma}
\begin{proof}
Consider the first $b$ letters of $X$ as a sequence of pairs, given by
$(x_1, x_2), \ldots, (x_{b - 1}, x_b)$. If at least half of the pairs
$(x_i, x_{i + 1}$ satisfy $x_i \neq x_{i + 1}$, then the encode
function can reorder those pairs to appear at the front of $X$, and
then use them to encode $v$ as in Lemma \ref{lem:bitstore}. Note that
the reordering of the pairs will only be performed the first time that
the encode function is invoked on $X$. Later calls to the encode
function will have work $O(\log n)$ and span $O(\log \log n)$, as in
Lemma \ref{lem:bitstore}.

If, on the other hand, at least half the pairs consist of equal-value
elements $x_i = x_{i + 1}$, then we can reorder the pairs so that the
first $\lceil \log (n + 1) \rceil + 1$ of them satisfy this
property. (This is only done on the first call to encode.) To encode a
value $v$, we simply explicitly overwrite the second element in each
of the pairs $(x_3, x_4), (x_5, x_6), \ldots$ with the bits of $v$,
overwriting each element with one bit. The reordering performed by the
first call to encode has work and span $O(\log n)$; the writing of
$v$'s bits can then be performed in work $O(\log n)$ and span $O(\log
\log n)$ using a simple divide-and-conquer approach.

To perform a decode and read the value $v$, we check whether $x_1 =
x_2$ in order to determine which type of encoding is being used, and
then we can unencode the bits of $v$ using work $O(\log n)$ and span
$O(\log \log n)$; if the encoding is the second type (i.e., $x_1 =
x_2$), then the decode function also restores the elements $x_2, x_4,
x_6, \ldots$ of the array $X$ as it extracts the bits of $v$. Note
that checking whether $x_1 = x_2$ is also used by the encode function
each time after the first time it is called, in order determine which
type of encoding is being used.
\end{proof}

The fact that the first call to the encode function on each $X_i$ has
span $O(\log n)$ (rather than $O(\log \log n)$) does not affect the
total span of our parallel-partition algorithm, since this simply adds
a step with $O(\log n)$-span to the beginning of the Parallel Prefix
phase. Lemma \ref{lem:bitstore2} can therefore used in place of Lemma
\ref{lem:bitstore} in order to complete the proof of Theorem
\ref{thminplace} for arrays $A$ that contain duplicate elements.


% hybrid
\section{A Cache-Efficient $O(\log n \log \log n)$-Span Parallel Partition}
\label{sec:hybridSmoothedStriding}

In this section we analyze the \defn{Hybrid Smoothed Striding
Algorithm}, which, after the Partial Partition Step of the
Smoothed Strided Algorithm, partitions the sub-array using the
In-Place Sum-and-Swap Algorithm. We show that doing so results in
an improved span (since the In-Place Sum-and-Swap Algorithm has
span only $O(\log n \log \log n)$), while still incurring only
$(1 + o(1))n/b$ cache misses (since the cache-inefficient
In-Place Sum-and-Swap Algorithm is only used on a small subarray
of $A$). 

We now analyze the Hybrid Smoothed Striding Algorithm. We prove
the following theorem:
\begin{theorem}
  \label{thm:fullPartition} The Hybrid Smoothed Striding
  Algorithm using parameter $\delta\in(0,1/2)$ satisfying $\delta
  \ge 1/\polylog(n)$: achieves work $O(n)$; achieves span
  $$O\left(\log n \log\log n +\frac{b\log n}{\delta^2}\right),$$ with
  high probability in $n$; and incurs fewer than
  $$(n+O(n\delta))/b$$ cache misses with high probability in $n$.
\end{theorem}

%% note: b/loglogn > 1 in reality, but the constant factors make it so that delta is still < 1

An interesting corollary of Theorem \ref{thm:fullPartition}
concerns what happens when $b$ is small (e.g., constant) and we
choose $\delta$ to optimize span:

%% This can be done with an extreme setting of $\delta$.
%% This is interesting because it shows that it is possible to achie
%%ve low span along with a small number of cache misses.
\begin{corollary}[Corollary of Theorem \ref{thm:fullPartition}]
  \label{cor:fullPartition}
Suppose $b \le o(\log \log n)$. Then the Hybrid Smoothed Striding
using $\delta = \Theta\big(\sqrt{b/\log\log n}\big)$, achieves
work $O(n)$, and with high probability in $n$, achieves span
$O(\log n \log\log n)$ and incurs fewer than $(n+o(n))/b$ cache misses.
\end{corollary}

\begin{proof}[Proof of Theorem \ref{thm:fullPartition}]
  We analyze the Partial Partition Step using Proposition
  \ref{prop:generalResult}. Note that by our choice of $\epsilon$,
  $s=O\left(\frac{\log n}{\delta^2}\right)$.  The Partial Partition
  Step therefore has work $O(n)$, span $O\left(\frac{b\log
  n}{\delta^2}\right),$ and incurs fewer than
  $$\frac{n}{b}+O\left(\frac{\log n}{b\delta^2}\right)+O(1)$$ 
  cache misses.

  By Theorem \ref{thminplace}, the subproblem of partitioning of
  $A[v_{\text{min}} + 1], \ldots, A[v_{\text{max}}]$ takes work
  $O(n)$. With high probability in $n$, the subproblem has size
  less than $4n\delta$, which means that the subproblem achieves
  span $$O(\log n\delta \log\log n\delta) = O(\log n \log\log
  n),$$ and incurs at most $O(n \delta / b)$ cache misses.

  The total number of cache misses is therefore,
  $$\frac{n}{b}+O\left(\frac{\log n}{b\delta^2} +
  \frac{n\delta}{b}\right)+O(1),$$ which since $\delta \ge 1 /
  \polylog(n)$, is at most $(n+O(n\delta))/b + O(1) \le (n + O(n
  \delta)) / b,$ as desired.
\end{proof}

\begin{proof}[Proof of Corollary \ref{cor:fullPartition}] We use
  $\delta = \sqrt{b/\log\log n}$ in the result proved in Theorem
  \ref{thm:fullPartition}. \\
  First note that the assumptions of Theorem
  \ref{thm:fullPartition} are satisfied because
  $O(\sqrt{b/\log\log n}) > 1 / \polylog(n).$
  The algorithm achieves work $O(n)$. 
  With high probability in $n$ the algorithm achieves span 
  $$O\left(\log n \log\log n +\frac{b\log n}{\delta^2}\right) = O(\log n\log\log n).$$
  With high probability in $n$ the algorithm incurs fewer than 
  $$(n+O(n\delta))/b = (n+O(n\sqrt{b/\log\log n}))/b$$ 
  cache misses.
  By assumption $\sqrt{b/\log\log n} = o(1)$, so this reduces to 
  $(n+o(n))/b$
  cache misses, as desired.
\end{proof}


% conclusion
\section{Conclusion and Open Questions}\label{sec:open}

Parallel partition is a fundamental primitive in parallel algorithms
\cite{Blelloch96,AcarBl16}. Achieving faster and more space-efficient
implementations, even by constant factors, is therefore of high
practical importance. Until now, the only space-efficient algorithms
for parallel partition have relied extensively on concurrency
mechanisms or atomic operations, or lacked provable performance
guarantees. If a parallel function is going to be invoked within a large
variety of applications, then provable guarantees are highly
desirable. Moreover, algorithms that avoid the use of concurrency
mechanisms tend to scale more reliably (and with less dependency on
the particulars of the underlying hardware).

In this paper, we have shown that, somewhat surprisingly, one can
adapt the classic parallel algorithm to completely eliminate the
use of auxiliary memory, while still using only exclusive
read/write shared variables, and maintaining a polylogarithmic
span. Although the superior cache performance of the low-space
algorithm results in practical speedups over its out-of-place
counterpart, both algorithms remain far from the state-of-the art
due to memory bandwidth bottlenecks. To close this gap, we also
presented a second in-place algorithm, the Smoothed Striding
Algorithm, which achieves polylogarithmic span while guaranteeing
provably optimal cache performance up to low-order factors. The
Smoothed Striding Algorithm introduces randomization techniques
to the previous (blocked) Striding Algorithm of \cite{Frias08,
FrancisPa92}, which was known to perform well in practice but
which previously exhibited poor theoretical guarantees. Our
implementation of the Smoothed Striding Algorithm is fully
in-place, exhibits polylogarithmic span, and has optimal cache
performance.

Our work prompts several theoretical questions. Can fast
space-efficient algorithms with polylogarithmic span be found for
other classic problems such as randomly permuting an array
\cite{Anderson90, AlonsoSc96, ShunGu15}, and integer sorting
\cite{Rajasekaran92, HanHe12, AlbersHa97, Han01, GerbessiotisSi04}?
Such algorithms are of both theoretical and practical interest, and
might be able to utilize some of the techniques introduced in this
paper.

Another important direction of work is the design of in-place parallel
algorithms for sample-sort, the variant of quicksort in which multiple
pivots are used simultaneously in each partition. Sample-sort can be
implemented to exhibit fewer cache misses than quicksort, which is
especially important when the computation is memory-bandwidth
bound. The known in-place parallel algorithms for sample-sort rely
heavily on atomic instructions \cite{AxtmannWi17} (even requiring
128-bit compare-and-swap instructions). Finding fast algorithms that
use only exclusive-read-write memory (or
concurrent-read-exclusive-write memory) is an important direction of
future work.

\clearpage

\appendix

% \section{Performance Comparisons}\label{secexp}

% In this section, we implement the techniques from
% Section~\ref{sec:blockedprefixsumpartitionalg} and 
% Section~\ref{sec:recursiveSmoothedStriding} to build space-efficient and
% in-place parallel-partition functions.

% Each implementation considers an array of $n$ 64-bit integers, and
% partitions them based on a pivot. The integers in the array are
% initially generated so that each is randomly either larger or smaller
% than the pivot.

% In Subsection~\ref{subsecclassic}, we evaluate the techniques in
% Section~\ref{sec:blockedprefixsumpartitionalg} for transforming the standard
% parallel-prefix-based partition algorithm into an in-place
% algorithm. We compare the performance of three parallel-partition
% implementations: (1) The \defn{high-space} implementation which
% follows the standard parallel-partition algorithm exactly; (2) a
% \defn{medium-space} implementation which reduces the space used for
% the Parallel-Prefix phase; and (3) a \defn{low-space} implementation
% which further eliminates the auxiliary space used in the Reordering
% phase of the algorithm. The low-space implementation still uses a
% small amount of auxiliary memory for the parallel-prefix, storing
% every $O(\log n)$-th element of the parallel-prefix array explicitly
% rather than using the implicit-storage approach in Section~\ref{sec:blockedprefixsumpartitionalg}. 
% Nonetheless the space consumption is several orders of
% magnitude smaller than the original algorithm.

% In addition to achieving a space-reduction, the better cache-behavior
% of the low-space implementation allows for it to achieve a speed
% advantage over its peers, in some cases completing roughly twice as
% fast as the medium-space implementation and four times as fast as the
% low-space implementation. We show that all three implementations are
% bottlenecked by memory throughput, however, suggesting that the cache-optimal Smoothed Striding Algorithm can do better.

% In Subsection \ref{subsecstrided}, we evaluate the performance of the
% Recursive Smoothed Striding Algorithm and the Strided
% Algorithm. Unlike the algorithms described above, the implementations
% of both of these algorithms are fully in-place, meaning that the total
% space overhead is only $\polylog n$. The cache efficiency of these two
% algorithms allows for them to achieve substantially better scaling
% than their parallel-prefix-based counterparts. The Strided Algorithm
% tends to slightly outperform the Smoothed Striding Algorithm, though
% on 18 threads their performance is within 15\% of one-another. We
% conclude that the Smoothed Striding Algorithm allows for one to obtain
% empirical performance comparable to that of the Strided Algorithm,
% while simultaneously achieving the provable guarantees on span and
% cache-efficiency missing from the original Strided Algorithm.


% %% In Subsection \ref{subsechighspan}, we present a fourth
% %% parallel-partition algorithm which we call the \defn{two-layer
% %%   algorithm}, and which runs fully in place but has a polynomial span
% %% of $\Theta(\sqrt{n \log n})$. The polynomial span of the algorithm
% %% makes it so that a naive implementation performs poorly. Nonetheless,
% %% we show that by tuning the algorithm to the number of worker threads,
% %% further speedup can often be achieved over the low-space algorithm.

% %% The two-layer algorithm has the advantage that is very simple to
% %% implement, and runs in serial at almost the same speed as GNU Libc
% %% quicksort's serial algorithm. On the other hand the algorithm's
% %% performance is much more sensitive to input size and number of cores
% %% than is the low-space implementation. On a machine with sufficiently
% %% many cores (and sufficiently large memory bandwidth), the
% %% polylogarithmic span of the low-space implementation is desirable.


% \paragraph{Machine Details}
% Our experiments are performed on a two-socket machine with eighteen
% 2.9 GHz Intel Xeon E5-2666 v3 processors. To maximize the memory
% bandwidth of the machine, we use a NUMA memory-placement policy in
% which memory allocation is spread out evenly across the nodes of the
% machine; this is achieved using the \emph{interleave=all} option in
% the Linux \emph{numactl} tool \cite{Kleen05}. Worker threads in our
% experiments are each given their own core, with hyperthreading
% disabled.

% Our algorithms are implemented using the CilkPlus task parallelism
% library in C++. The implementations avoid the use of concurrency
% mechanisms and atomic operations, but do allow for concurrent reads to
% be performed on shared values such as $n$ and the pointer to the input
% array. Our code is compiled using g++ 7.3.0, with \emph{march=native}
% and at optimization level three. 

% Our implementations are available on GitHub.


% \subsection{Comparing Parallel-Prefix-Based Algorithms}\label{subsecclassic}

% In this section, we compare four partition implementations,
% incorporating the techniques from Section~\ref{sec:blockedprefixsumpartitionalg} in order to
% achieve space efficiency:
% \begin{itemize}[leftmargin = .15in]
% \item \emph{A Serial Baseline:} This uses the serial in-place
%   partition implementation from GNU Libc quicksort, with minor
%   adaptations to optimize it for the case of sorting 64-bit integers
%   (i.e., inlining the comparison function, etc.).
% \item \emph{The High-Space Parallel Implementation:} This uses the
%   standard parallel partition algorithm \cite{Blelloch96,AcarBl16}, as
%   described in Section \ref{secprelim}. The space overhead is roughly
%   $2n$ eight-byte words.
% \item \emph{The Medium-Space Parallel Implementation:} Starting with
%   the high-space implementation, we reduce the space used by the
%   Parallel-Prefix phase by only constructing every $O(\log n)$-th
%   element of the prefix-sum array $B$, as in Section~\ref{sec:blockedprefixsumpartitionalg}. (Here $O(\log n)$ is hard-coded as 64.) The array $B$
%   is initialized to be of size $n / 64$, with each component equal to
%   $\sum_{i=1}^{64} \dec(A[64 (i-1)+1])$, and then a parallel prefix sum is computed on
%   the array $B$. Rather than implicitly encoding the elements of $B$ in
%   $A$, we use an auxiliary array of size $n / 64$ to explicitly store
%   the prefix sums.

%   %% \footnote{We suspect that an implementation in which
%   %%   the values are implicitly stored could also be made fast. In
%   %%   particular, the value 64 can be increased to compensate for
%   %%   whatever constant overhead is induced by the implicit storage of
%   %%   values. Nonetheless, the auxiliary array is already quite small
%   %%   relative to the input and is more practical to implement.}
%   The algorithm
%   has a space overhead of $\frac{n}{32} + n$ eight-byte
%   words.\footnote{In addition to the auxiliary array of size $n / 64$,
%     we use a series of smaller arrays of sizes $n / 128, n / 256,
%     \ldots$ in the recursive computation of the prefix sum. The
%     alternative of performing the parallel-prefix sum in place, as in
%     Section~\ref{sec:blockedprefixsumpartitionalg}, tends to be less cache-friendly in
%     practice.}
% \item \emph{The Low-Space Parallel Implementation:}
% Starting with the medium-space implementation, we make the reordering
% phase completely in-place using the preprocessing technique in Section~\ref{sec:blockedprefixsumpartitionalg}.
% \footnote{Depending on whether the majority of elements
%   are predecessors or successors, the algorithm goes down separate
%   (but symmetric) code paths. In our timed experiments we test only
%   with inputs containing more predecessors than successors, since this
%   the slower of the two cases (by a very slight amount) for our
%   implementation.} The only space overhead in this implementation is
% the $\frac{n}{32}$ additional 8-byte words used in the prefix sum.
% \end{itemize}

% %% All three parallel-implementations have work $O(n)$ The high- and
% %% medium- space implementations have span $O(\log n)$, while the
% %% low-space implementation has span $O(\log^2 n)$ (due to the fact that
% %% for convenience of implementation parallel-for-loops are broken into
% %% chunks of size $64 = O(\log n)$).

% We remark that the ample parallelism of the low-space algorithm makes
% it so that for large inputs the value $64$ can easily be increased
% substantially without negatively effecting algorithm performance. For
% example, on an input of size $2^{28}$, increasing it to $4096$ has
% essentially no effect on the empirical runtime while bringing the
% auxiliary space-consumption down to a $\frac{1}{2048}$-fraction of the
% input size. (In fact, the increase from 64 to 4096 results in roughly
% a 5\% speedup.)

% \paragraph{An Additional Optimization for The High-Space Implementation}
% The optimization of reducing the prefix-sum by a factor of $O(\log n)$
% at the top level of recursion, rather than simply by a factor of two,
% can also be applied to the standard parallel-prefix algorithm when
% constructing a prefix-sum array of size $n$. Even without the space
% reduction, this reduces the (constant) overhead in the parallel prefix
% sum, while keeping the overall span of the parallel-prefix operation
% at $O(\log n)$. We perform this optimization in the high-space
% implementation.

% \paragraph{Performance Comparison}
% Figure \ref{tablecilk} graphs the speedup of the each of the parallel
% algorithms over the serial algorithm, using varying numbers of worker
% threads on an 18-core machine with a fixed input size of $n =
% 2^{30}$. Both space optimizations result in performance improvements,
% with the low-space implementation performing almost twice as well as
% the medium-space implementation on eighteen threads, and almost four
% times as well as the high-space implementation. %% Similar speedups are
% %% achieved on smaller inputs; see Figure \ref{tablecilk2}, which graphs
% %% speedup for input sizes starting at $2^{23}$.

% Figure \ref{tableserial} compares the performances of the
% implementations in serial. Parallel-for-loops are replaced with serial
% for-loops to eliminate scheduler overhead. As the input-size varies,
% the ratios of the runtimes vary only slightly. The low-space
% implementation performs within a factor of roughly 1.9 of the serial
% implementation. As in Figure \ref{tablecilk},
% both space optimizations result in performance improvements.

% \paragraph{The Source of the Speedup}
% If we compare the number of instructions performed by the three
% parallel implementations, then the medium-space algorithm would seem
% to be the clear winner. Using Cachegrind to profile the number of
% instructions performed in a (serial) execution on an input of size
% $2^{28}$,\footnote{This smaller problem size is used to compensate for the fact that Cachegrind can be somewhat slow.} the high-space, medium-space, and low-space implementations
% perform 4.4 billion, 2.9 billion, and 4.6 billion instructions,
% respectively.

% Cache misses tell a different story, however. Using Cachegrind to
% profile the number of top-level cache misses in a (serial) execution
% on an input of size $2^{28}$, the high-space, medium-space, and
% low-space implementations incur 305 million, 171 million, and 124
% million cache misses, respectively.

% To a first approximation, the number of cache misses by each algorithm
% is proportional to the number of times that the algorithm scans
% through a large array. By eliminating the use of large auxiliary
% arrays, the low-space implementation has the opportunity to achieve a
% reduction in the number of such scans. Additionally, the low-space
% algorithm allows for steps from adjacent phases of the algorithm to
% sometimes be performed in the same pass. For example, the enumeration
% of the number of predecessors and the top level of the Preprocessing
% phase can be performed together in a single pass on the input
% array. Similarly, the later levels of the Preprocessing phase (which
% focus on only one half of the input array) can be combined with the
% construction of (one half of) the auxiliary array used in the Parallel
% Prefix Sum phase, saving another half of a pass.


% %% There are additional slightly more tricky potential optimizations
% %% that we did not make. One could, for example, also combine the
% %% construction of the other half of the auxiliary array used in the
% %% Parallel Prefix Sum Phase with the top level of the Preprocessing
% %% Phase; this is made more subtle by the fact that when performing the
% %% top level of the Preprocessing Phase, we do not know yet whether we
% %% will be recursing on the left or right half of the array. One could
% %% also implement the later levels of the Preprocessing Phase to have a
% %% more cache-friendly recursive structure, starting with the final step
% %% of the Preprocessing Phase and recursively working backwards to
% %% perform the entire phase in a depth-first recursive tree. Evaluating
% %% these optimizations would be an interesting direction of future work.

% \paragraph{The Memory-Bandwidth Limitation}
% The comparison of cache misses suggests that performance is
% bottlenecked by memory bandwidth. To evaluate whether this is the
% case, we measure for each $t \in \{1, \ldots, 18\}$ the memory
% throughput of $t$ threads attempting to scan through disjoint portions
% of a large array in parallel. We measure two types of bandwidth, the
% \defn{read-bandwidth}, in which the threads are simply trying to read
% from the array, and the \defn{read/write bandwidth}, in which the
% threads are attempting to immediately overwrite entries to the array
% after reading them. Given read-bandwidth $r$ bytes/second and
% read/write bandwidth $w$ bytes/second, the time needed for the
% low-space algorithm to perform its memory operations on an input of
% $m$ bytes will be roughly $3.5 m / w + .5m / r$ seconds.\footnote{A naive implementation of the algorithm would require roughly $m / r$ time to count the number of predecessors, followed by $2m / w$ time to perform the Preprocessing Phase, followed by roughly $m / r$ time to perform the Parallel Prefix Sum Phase, and then roughly $1.5 m/w$ time for the In-Place Reordering Phase. As described in the previous paragraph, however, the counting of predecessors and the Parallel Prefix Sum phase can both be overlapped with the Preprocessing phase so that their total added contribution to the Memory-Bandwidth Limitation is only $.5 m / r$.}  We call this
% the \defn{bandwidth constraint}. No matter how optimized the
% implementation of the low-space algorithm is, the bandwidth constraint
% serves as a hard lower bound for the running
% time.\footnote{Empirically, on an array of size $n = 2^{28}$, the total number of cache misses is within
%   $8\%$ of what this assumption would predict, suggesting that the
%   bandwidth constraint is within a small amount of the true
%   bandwidth-limited runtime.}

% Figure \ref{tablebandwidth} compares the time taken by the low-space
% algorithm to the bandwidth constraint as the number of threads $t$
% varies from $1$ to $18$. As the number of threads grows, the algorithm
% becomes bandwidth limited, achieving its best possible parallel
% performance on the machine. The algorithm scales particularly well on
% the first socket of the machine, achieving a speedup on nine cores of
% roughly six times better than its performance on a single core, and
% then scales more poorly on the second socket as it becomes
% bottlenecked by memory bandwidth.

% \paragraph{Implementation Details}
% In each implementation, the parallelism is achieved through simple
% parallel-for-loops, with one exception at the beginning of the
% low-space implementation, when the number of predecessors in the input
% array is computed. Although CilkPlus Reducers (or OpenMP Reductions)
% could be used to perform this parallel summation within a
% parallel-for-loop \cite{FrigoLe09}, we found a slightly more ad-hoc
% approach to be faster: Using a simple recursive structure, we manually
% implemented a parallel-for-loop with Cilk Spawns and Syncs, allowing
% for the summation to be performed within the recursion.


% \subsection{Comparing the Smoothed Striding and Strided Algorithms} \label{subsecstrided}

% In this section we consider the performance of the Strided Algorithm
% and the Recursive Smoothed Striding Algorithm. Past work
% \cite{Frias08} found that, on large numbers of threads, the Strided
% Algorithm has performance close to that of other non-EREW state-of-the
% art partition algorithms (i.e., within 20\% of the best
% atomic-operation based algorithms). The Strided Algorithm does not
% offer provable guarantees on span and cache-efficiency, however; and
% indeed, the reason that the algorithm cannot recurse on the subarray
% $A[v_{\text{min}} + 1], \ldots, A[v_{\text{max}}]$ is that the subarray has been
% implicitly constructed to be worst-case for the algorithm. In this
% subsection, we show that, with only a small loss in performance, the
% Smoothed Striding Algorithm can be used to achieve provable guarantees
% on arbitrary inputs. We remark that we do not make any attempt to
% generate worst-case inputs for the Strided Algorithm (in fact the
% random inputs that we use are among the only inputs for which the
% Strided Algorithm does exhibit provable guarantees!).

% Figures \ref{tableserial} and \ref{tablecilk} evaluate the performance
% of the Smoothed Striding and Strided algorithms in serial and in
% parallel. On a single thread, the Smoothed Striding and Strided
% algorithms perform approximately 1.5 times slower than the Libc-based
% serial implementation baseline. When executed on multiple threads, the
% performances of the Smoothed Striding and Strided Algorithms scale
% close to linearly in the number of threads. On 18 threads, the
% Smoothed Striding Algorithm achieves a $9.6 \times $ speedup over the
% Libc-based Serial Baseline, and the Strided Algorithm achieves an
% $11.1 \times$ speedup over the same baseline.

% The nearly-ideal scaling of the two algorithms can be explained by
% their cache behavior. Whereas the parallel-prefix-based algorithms
% were bottlenecked by memory bandwidth, Figure \ref{tablebandwidth}
% shows that the same is no longer true for the Smoothed Striding
% Algorithm. The figure compares the performance of the Smoothed
% Striding Algorithm to the minimum time needed simple to read and overwrite
% each entry of the input array using $18$ concurrent threads without any other
% computation (i.e., the memory bandwidth constraint). On 18 threads, the time required by the memory bandwidth
% constraint constitutes $58\%$ of the algorithm's total running time.

% \paragraph{NUMA Effects}
% We remark that the use of the Linux \emph{numactl} tool \cite{Kleen05}
% to spread memory allocation evenly across the nodes of the machine is
% necessary to prevent the Smoothed Striding Algorithm and the Strided
% Algorithm from being bandwidth limited. For example, if we replicate
% the 18-thread column of Figure \ref{tablebandwidth} without using
% \emph{numactl}, then the speedup of the Smoothed Striding Algorithm is
% 8.2, whereas the memory-bandwidth bound for maximum possible speedup
% is only slightly larger at $10.2$.


% \paragraph{Implementation Details} Both algorithms use $b = 512$. The Smoothed Striding Algorithm uses
% slightly tuned $\epsilon, \delta$ parameters similar to those outlined
% in Corollary \ref{cor:groupedPartitionAlg}. Although $v_{\text{min}}$
% and $v_{\text{max}}$ could be computed using CilkPlus Reducers
% \cite{FrigoLe09}, we found it advantageous to instead manually
% implement the parallel-for-loop  in the Partial Partition step with Cilk Spawns and Syncs, and to
% compute $v_{\text{min}}$ and $v_{\text{max}}$ within the recursion.

% \paragraph{Example Application: A Full Quicksort}
% In Figure \ref{tablesort}, we graph the performance of a parallel
% quicksort implementation using the low-space parallel-prefix-based
% algorithm, the Smoothed Striding Algorithm, and the Strided
% Algorithm. We compare the algorithm performances 
% with varying numbers of worker threads and input sizes to GNU Libc quicksort; the input
% array is initially in a random permutation.

% Our parallel quicksort uses the parallel-partition algorithm at the
% top levels of recursion, and then swaps to the serial-partitioning
% algorithm once the input size has been reduced by at least a factor of
% $8p$, where $p$ is the number of worker threads. By using the
% serial-partitioning algorithm on the small recursive subproblems we
% avoid the overhead of the parallel algorithm, while still achieving
% parallelism between subproblems. Small recursive problems also exhibit
% better cache behavior than larger ones, reducing the effects of
% memory-bandwidth limitations on the performance of the parallel
% quicksort, and further improving the scaling.



%% \subsection{An In-Place Algorithm with Polynomial Span}\label{subsechighspan}

%% In this subsection, we consider a simple in-place parallel-partition
%% algorithm with polynomial span. We evaluate the algorithm as a simple
%% and even-lower-overhead alternative to the low-space algorithm in the
%% previous subsection.

%% The algorithm takes two steps:
%% \begin{itemize}
%% \item \textbf{Step 1:} The algorithm breaks the input array $A$ into
%%   $t$ equal-sized parts, $P_1, \ldots, P_t$, for some parameter $t$. A
%%   serial partition is performed on each of the $P_i$'s in
%%   parallel. This step takes work $\Theta(n)$ and span $\Theta(n / t)$.
%% \item \textbf{Step 2:} The algorithm loops in serial through each of
%%   the $t$ parts $P_1, \ldots, P_t$. Upon visiting $P_i$, the algorithm
%%   has already performed a complete partition on the subarray $P_1
%%   \circ \cdots \circ P_{i - 1}$. Let $j$ denote the number of
%%   predecessors in $P_1, \ldots, P_{i - 1}$, and $k$ denote the number
%%   of predecessors in $P_i$. The algorithm computes $k$ through a
%%   simple binary search in $P_i$. The algorithm then moves the $k$
%%   elements at the start of $P_i$ to take the place of the $k$ elements
%%   $A[j + 1], \ldots, A[j + k]$. If the two sets of $k$ elements are
%%   disjoint, then they are swapped with one-another in a
%%   parallel-for-loop; otherwise, the non-overlapping portions of the
%%   two sets of $k$ elements are swapped in parallel, while the
%%   overlapping portion is left untouched. This completes the
%%   partitioning of the parts $P_1 \circ \cdots \circ P_i$. Performing
%%   this step for $i = 1, \ldots, t$ requires work $O(t \log n + n)$ and
%%   span $\Theta(t \log n)$.
%% \end{itemize}

%% Setting $t = \sqrt{n}$, the algorithm runs in linear time with span
%% $\sqrt{n} \log n$; refining $t$ to an optimal value of $\sqrt{n / \log
%%   n}$ results a span of $\sqrt{n \log n}$. In practice,
%% however, this leaves too little parallelism in the parallel-for-loops
%% in Step 2, resulting in poor scaling.\footnote{On 18 threads an on an
%%   input of size $2^{28}$, for example, setting $t = \sqrt{n}$ results
%%   in a performance a factor of two slower than the low-space
%%   implementation, and setting $t = \sqrt{n / \log n}$ makes only
%%   partial progress towards closing that gap.} To mitigate this, we
%% tune our implementation of the algorithm to the number of processors
%% $p$ on which it is being run, setting $t = 8 \cdot p$, in order to
%% maximize the parallelism in the for-loops in Step 2, while still
%% providing sufficient parallelism for Step 1.

%% Figures \ref{tablecilk} and \ref{tablecilk2} compare the parallel
%% performance of the algorithm, which is referred to as the
%% \defn{two-layer algorithm}, to its lower-span peers. On 18 cores and
%% on an input of size $2^{28}$, the two-layer algorithm offers a speedup of
%% roughly 50\% over the low-space algorithm. The algorithm is more
%% sensitive to input-size and number of cores, however, requiring a
%% large enough ratio between the two to compensate for the algorithm's
%% large span (See Figure \ref{tablecilk2}).

%% Figure \ref{tableserial} compares the performance of the two-layer
%% algorithm in serial to GNU Libc quicksort. The algorithm runs within a
%% small fraction (less than $1/4$) of the serial implementation.

%% Figure \ref{tablebandwidth} evaluates the degree to which the
%% algorithm is memory-bandwidth bound on an input of size $2^{28}$. If
%% the read/write bandwidth of the algorithm is $w$ bytes/second, then
%% the bandwidth constraint for the algorithm on an input of $m$ bytes is
%% given by $2m / w$. In particular, Step 1 of the algorithm makes one
%% scan through the array, requiring time $m / w$; and then Step 2
%% rearranges the predecessors (which constitute half of the array and
%% each must be moved to a new location), requiring additional time $m /
%% w$. Figure \ref{tablebandwidth} compares the time taken by the
%% algorithm to the bandwidth constraint as the number of threads $t$
%% varies from $1$ to $18$. Like the low-space algorithm, as the number
%% of threads grows, the algorithm becomes close to bandwidth limited.

%% Figure \ref{tablesort} compares the performance of a quicksort
%% implementation using the two-layer partition algorithm, to the
%% performance of an implementation using the low-space algorithm. The
%% implementation using the two-layer algorithm achieves a modest speedup
%% over the low-space algorithm, but also demonstrates its larger span by
%% suffering on smaller inputs as the number of cores grows.



\section{Pseudocode}\label{sec:pseudocode}

\begin{figure*}[h]
  \scriptsize
  \caption{Smoothed Striding Algorithm}
	\label{alg:parallelPartition_smoothedStriding}
	\begin{algorithmic}% [1] for line numbers
    \State \textbf{Recall:} 
    \State $A$ is the array to be partitioned, of length $n$. 
    \State We break $A$ into chunks, each consisting of $g$ cache lines of size $b$.
    \State We create $g$ groups $U_1,\ldots, U_g$ that each contain a single cache line from each chunk,
    \State $U_i$'s $j$-th cache line is the $(X[j]+i \bmod g + 1)$-th cache line in the $j$-th chunk of $A$.
    \State

    \Procedure{GetBlockStartIndex}{$X$, $g$, $b$, $i$, $j$}
      \Comment This procedure returns the index in $A$ of the start of $U_i's$ $j$-th block.
      \State\Return $b\cdot ((X[j] + i \bmod g) +(j-1)\cdot g)+1$
    \EndProcedure
    \State

    \Procedure{PartialPartition}{$A$, $n$, pivotValue, $g$, $b$}
      \For{$j \in \{1,2,\ldots,n/(gb)\}$}
        \State $X[j] \gets$ a random integer from $[1,g]$ 
      \EndFor

      \ForAll{$ i \in \{1,2,\ldots,g\}$ in parallel} \Comment We perform a serial partition on all $U_i$'s in parallel

        \State low $\gets$ GetBlockStartIndex($X$,$g$,$b$,$i$,$1$)
        \Comment low $\gets$ index of the first element in $U_i$
        \State high $\gets$ GetBlockStartIndex($X$,$g$,$b$,$i$,$n/(gb)$) + $b-1$
        \Comment high $\gets$ index of the last element in $U_i$

        \While{low $<$ high} 
          \While{A[low] $\leq$ pivotValue}
            \State low $\gets$ low$+1$
            \If{low $\bmod b \equiv 0$ } 
              \Comment Perform a block increment once low reaches the end of a block
              \State $k \gets $ number of block increments so far (including this one)
              \State low $\gets$ GetBlockStartIndex($X$,$g$,$b$,$i$,$k$)
              \Comment Increase low to start of block $k$ of $G_i$
            \EndIf
          \EndWhile
          \While{A[high] $>$ pivotValue}
            \State high $\gets$ high$-1$
            \If{high $\bmod b \equiv 1$} 
              \Comment Perform a block decrement once high reaches the start of a block
              \State $k \gets $ number of block decrements so far (including this one)
              \State $k' \gets n/(gb) - k$
              \State high $\gets$ GetBlockStartIndex($X$, $g$,$b$,$i$,$k'$) $+b-1$
              \Comment Decrease high to end of block $k'$ of $G_i$
            \EndIf
          \EndWhile
          \State Swap $A[\text{low}]$ and $A[\text{high}]$
        \EndWhile
      \EndFor
    \EndProcedure
    \State

    \Procedure{RecursiveSmoothedStriding}{$A$, $n$, pivotValue, $g$, $b$, $\delta$}
      \If{$g<2$}
        \State serial partition $A$
      \Else
        \State PartialPartition($A$, $n$, pivotValue, $g$, $b$)
        \State Partition $A[v_{min}],\ldots,A[v_{max}-1]$ with the Recursive Smoothed Striding Algorithm
      \EndIf
    \EndProcedure
    \State

    \Procedure{HybridSmoothedStriding}{$A$, $n$, pivotValue, $g$, $b$}
      \If{$g<2$}
        \State serial partition $A$
      \Else
        \State PartialPartition($A$, $n$, pivotValue, $g$, $b$)
        \State Partition $A[v_{min}],\ldots,A[v_{max}-1]$ with the In-Place Sum-and-Swap Algorithm
      \EndIf
    \EndProcedure

	\end{algorithmic}	
\end{figure*}

\begin{figure*}
  \scriptsize
  \caption{In-Place Sum-and-Swap Algorithm}
	\label{alg:parallelPartition_prefixsumbased_main}
  \begin{algorithmic} % add [1] to get line numbers (if you want)
    \Procedure{WriteToBlock}{$A$, $b$, $i$, $v$} \Comment Write value $v$ to the $i$-th block $X_i$ of $A$, where $A = X_1 \circ X_2 \circ \cdots \circ X_{\lfloor n/b \rfloor}$
      \ForAll{$j \in \{1,2,\ldots, \lfloor b/2 \rfloor \}$ in parallel}
      \If{$\mathds{1}_{X_i[2j] < X_i[2j+1]} \not= $ (the $j$-th binary digit of $v$)}
          \State Swap $X_i[2j]$ and $X_i[2j+1]$
        \EndIf
      \EndFor
    \EndProcedure
    \State

    \Procedure{ReadFromBlock}{$A$, $i$, $j$} \Comment Reads the value $v$ stored in $A[i], A[i+1], \ldots, A[j]$
      \If{$j-i=2$}
        \State \Return $\mathds{1}_{A[i] < A[i+1]}$
      \Else
        \State \emph{Parallel-Spawn} $v_0 \gets $ ReadFromBlock($A$, $i$, $i+(j-i)/2$)
        \State \emph{Parallel-Spawn} $v_f \gets $ ReadFromBlock($A$, $i+(j-i)/2+1$, $j$)
        \State \emph{Parallel-Sync}
        \State \Return $v_f\cdot 2^\frac{j-i}{4} + v_0$
      \EndIf
    \EndProcedure
    \State

    \State \textbf{Require: } $A$ has more successors than predecessors
    \State \textbf{Ensure: }  Each prefix of $A$ is ``successor heavy''
    \Procedure{MakeSuccessorHeavy}{$A$, $n$, pivotValue}
      \ForAll{$i \in \{1,2,\ldots,\lfloor n/2 \rfloor\}$ in parallel}
        \If{$A[i]$ is a predecessor and $A[n-i+1]$ is a successor}
          \State Swap $A[i]$ and $A[n-i+1]$
        \EndIf
      \EndFor
      \State MakeSuccessorHeavy($A$, $\lceil n/2 \rceil$, pivotValue)
      \Comment Recurse on $A[1],A[2], \ldots, A[\lceil n/2 \rceil]$
    \EndProcedure
    \State


    \State \textbf{Require: } Each prefix of $A$ is ``successor heavy''
    \State \textbf{Ensure: }  Each block $X_i$ stores how many predecessors occur in $X_1 \circ X_2 \circ \cdots \circ X_i$
    \Procedure{ImplicitParallelPrefixSum}{A, n, pivotValue}
      \State Pick $b \in \Theta(\log n)$ to be the ``block size''
      \State Logically partition $A$ into blocks, with $A = X_1 \circ X_2 \circ \cdots \circ X_{\lfloor n/b \rfloor}$
      \ForAll{$i \in \{1,2,\ldots,\lfloor n/b \rfloor\}$ in parallel}
        \State $v_i \gets 0$ \Comment{$v_i$ will store number of predecessors in $X_i$ } 
        \ForAll{$a \in X_i$ in serial}
          \If{$a$ is a predecessor}
            \State $v_i \gets v_i + 1$
          \EndIf
        \EndFor
        \State WriteToBlock($A$, $b$, $i$, $v_i$)
        \Comment Now we encode the value $v_i$ in the block $X_i$
      \EndFor
      \State Perform a parallel prefix sum on the values $v_i$ stored in the $X_i$'s
   \EndProcedure
    \State

  \State \textbf{Require: } Each block $X_i$ stores how many predecessors occur in $X_1 \circ X_2 \circ \cdots \circ X_i$
  \State \textbf{Ensure: }  $A$ is partitioned
    \Procedure{Reorder}{$A$, $n$, pivotValue}
      \State $t \gets $ least integer such that $t\cdot b > n\cdot 4/5$
      \State Reorder($A$, $t$, pivotValue)
      \Comment Recurse on $A[1], A[2], \ldots, A[t]$
      \ForAll{$i \in \{t+1, t+2, \ldots, \lfloor n/b \rfloor\}$}
      \State $v_i \gets$ ReadFromBlock($A$, $b\cdot i+1$, $b\cdot(i+1)$) 
        \State Instantiate an array $Y_i$ with $|Y_i| = |X_i| \in \Theta(\log n)$, 
        \State In parallel, set $Y_i[j] \gets 1$ if $X_i[j]$ is a predecessor, and $Y_i[j] \gets 0$ otherwise.
        \State Perform a parallel prefix sum on $Y_i$, and add $v_i$ to each $Y_i[j]$
        \ForAll{$j \in \{1,2,\ldots, b\}$}
          \If{$X_i[j]$ is a predecessor}
            \State Swap $X_i[j]$ and $A[Y_i[j]]$
          \EndIf
        \EndFor
      \EndFor
    \EndProcedure
    \State

    \Procedure{InPlaceSumAndSwap}{$A$, $n$, pivotValue}
      \State $k \gets$ count number of successors in $A$ in parallel
      \If{$k < n/2$}
        \State Swap the role of successors and predecessors in the algorithm (i.e. change the decider function)
        \State At the end we consider $A'[i] = A[n-i+1]$, the logically reversed array, as output
      \EndIf

      \State MakeSuccessorHeavy($A$, $n$, pivotValue) \Comment \emph{prepreocessing phase}
      \State ImplicitParallelPrefixSum($A$, $n$, pivotValue) \Comment \emph{Implicit Parallel Prefix Sum}
      \State Reorder($A$, $n$, pivotValue) \Comment \emph{In-Place Reordering Phase}
    \EndProcedure
	\end{algorithmic}	
\end{figure*}

\clearpage
\clearpage

\bibliographystyle{siam}
\bibliography{paper}

\end{document}
