\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{AcarBl16,Blelloch96}
\citation{Blelloch96,AcarBl16}
\citation{BlellochFi12}
\citation{HeidelbergerNo90,AxtmannWi17,TsigasZh03}
\citation{Vitter08}
\citation{HeidelbergerNo90,AxtmannWi17,TsigasZh03}
\citation{FrancisPa92}
\citation{Hagerup89}
\citation{Katajainen93}
\citation{BlellochLe98}
\citation{HeidelbergerNo90,AxtmannWi17,TsigasZh03,FrancisPa92,Frias08}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{Blelloch96,AcarBl16}
\citation{Blelloch96,AcarBl16}
\citation{Brent74}
\citation{BlumofeJo96,BlumofeLe99}
\citation{Hagerup89}
\citation{Blelloch96,AcarBl16}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{2}{section.2}}
\newlabel{secprelim}{{2}{2}{Preliminaries}{section.2}{}}
\citation{LiuKn05}
\@writefile{toc}{\contentsline {section}{\numberline {3}A Simple In-Place Algorithm}{3}{section.3}}
\newlabel{secalg}{{3}{3}{A Simple In-Place Algorithm}{section.3}{}}
\newlabel{thminplace}{{3.1}{4}{}{theorem.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Evaluation}{4}{section.4}}
\newlabel{secexp}{{4}{4}{Experimental Evaluation}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces We compare the performance of the low-space and high-span sorting implementations running on varying numbers of threads and input sizes. The $x$-axis is the number of worker threads, the $y$-axis is the multiplicative speedup when compared to the serial baseline, and the log-base-two size of the input is indicated for each curve in the key. Each time (including each serial baseline) is averaged over five trials.\relax }}{5}{figure.caption.5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tablesort}{{1}{5}{We compare the performance of the low-space and high-span sorting implementations running on varying numbers of threads and input sizes. The $x$-axis is the number of worker threads, the $y$-axis is the multiplicative speedup when compared to the serial baseline, and the log-base-two size of the input is indicated for each curve in the key. Each time (including each serial baseline) is averaged over five trials.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces For a fixed table-size $n = 2^{28}$, we compare each implementation's runtime to a serial baseline, which takes 0.96 seconds to complete (averaged over five trials). The $x$-axis plots the number of worker threads being used, and the $y$-axis plots the multiplicative speedup over the serial baseline. Each time is averaged over five trials.\relax }}{5}{figure.caption.6}}
\newlabel{tablecilk}{{2}{5}{For a fixed table-size $n = 2^{28}$, we compare each implementation's runtime to a serial baseline, which takes 0.96 seconds to complete (averaged over five trials). The $x$-axis plots the number of worker threads being used, and the $y$-axis plots the multiplicative speedup over the serial baseline. Each time is averaged over five trials.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces We compare the performance of the implementations running on eighteen worker threads on varying input sizes. The $x$-axis is the log-base-$2$ of the input size, and the $y$-axis is the multiplicative speedup when compared to the serial baseline. Each time (including each serial baseline) is averaged over five trials.\relax }}{6}{figure.caption.7}}
\newlabel{tablecilk2}{{3}{6}{We compare the performance of the implementations running on eighteen worker threads on varying input sizes. The $x$-axis is the log-base-$2$ of the input size, and the $y$-axis is the multiplicative speedup when compared to the serial baseline. Each time (including each serial baseline) is averaged over five trials.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces We compare the performance of the implementations in serial, with no scheduling overhead. The $x$-axis is the log-base-$2$ of the input size, and the $y$-axis is the multiplicative slowdown when compared to the serial baseline. Each time (including each serial baseline) is averaged over five trials.\relax }}{6}{figure.caption.8}}
\newlabel{tableserial}{{4}{6}{We compare the performance of the implementations in serial, with no scheduling overhead. The $x$-axis is the log-base-$2$ of the input size, and the $y$-axis is the multiplicative slowdown when compared to the serial baseline. Each time (including each serial baseline) is averaged over five trials.\relax }{figure.caption.8}{}}
\citation{Kleen05}
\citation{Blelloch96,AcarBl16}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces We compare the performances of the low-space and high-span parallel-partition algorithms to their ideal performance determined by memory-bandwidth constraints on inputs of size $2^{28}$. The $x$-axis is the number of worker threads, and the $y$-axis is the multiplicative speedup when compared to the serial baseline (which is computed by an average over five trials). Each data-point is averaged over five trials.\relax }}{7}{figure.caption.9}}
\newlabel{tablebandwidth}{{5}{7}{We compare the performances of the low-space and high-span parallel-partition algorithms to their ideal performance determined by memory-bandwidth constraints on inputs of size $2^{28}$. The $x$-axis is the number of worker threads, and the $y$-axis is the multiplicative speedup when compared to the serial baseline (which is computed by an average over five trials). Each data-point is averaged over five trials.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Comparing Low-Span Algorithms}{7}{subsection.4.1}}
\newlabel{subseclowspan}{{4.1}{7}{Comparing Low-Span Algorithms}{subsection.4.1}{}}
\citation{FrigoLe09}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}An In-Place Algorithm with Polynomial Span}{9}{subsection.4.2}}
\newlabel{subsechighspan}{{4.2}{9}{An In-Place Algorithm with Polynomial Span}{subsection.4.2}{}}
\citation{FrancisPa92}
\citation{Frias08}
\citation{Blelloch96,AcarBl16}
\citation{Anderson90,AlonsoSc96,ShunGu15}
\citation{Rajasekaran92,HanHe12,AlbersHa97,Han01,GerbessiotisSi04}
\citation{AxtmannWi17}
\bibstyle{ACM-Reference-Format}
\bibdata{paper}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{4.62497pt}
\newlabel{tocindent2}{11.81937pt}
\newlabel{tocindent3}{0pt}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion and Open Questions}{10}{section.5}}
\newlabel{TotPages}{{10}{10}{}{page.10}{}}
