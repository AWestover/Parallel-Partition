\documentclass[xcolor=x11names, svgnames, rgb]{beamer}

\setbeamertemplate{navigation symbols}{}
\setbeamercolor{block title}{bg=blue!40}
\setbeamercolor{block body}{bg=blue!20}

%% Beamer Layout %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\useoutertheme[subsection=false,shadow]{miniframes}
\useinnertheme{default}
\usefonttheme{serif}
\usepackage{palatino}
\setbeamerfont{title like}{shape=\scshape}
\setbeamerfont{frametitle}{shape=\scshape}
\setbeamercolor*{lower separation line head}{bg=DeepSkyBlue4}
\setbeamercolor*{normal text}{fg=black,bg=white}
\setbeamercolor*{alerted text}{fg=red}
\setbeamercolor*{example text}{fg=black}
\setbeamercolor*{structure}{fg=black}
\setbeamercolor*{palette tertiary}{fg=black,bg=black!10}
\setbeamercolor*{palette quaternary}{fg=black,bg=black!10}
%% END Beamer Layout %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
\usepackage{algpseudocode}

\usepackage{mathtools}
\newcommand{\defeq}{\vcentcolon=}
\DeclarePairedDelimiter{\paren}{(}{)}

\newcommand{\dec}{\operatorname{dec}}
\newcommand{\poly}{\operatorname{poly}}
\newcommand{\polylog}{\operatorname{polylog}}
\newcommand{\github}{\url{github.com/awestover/Parallel-Partition}}
\newcommand{\defn}[1]       {{\textit{\textbf{\boldmath #1}}}}
\newcommand{\paragraph}[1]{\vspace{0.09in}\noindent{\bf \boldmath #1.}} 
\usepackage{amsmath}
\def\E{\operatorname{\mathbb{E}}}
\usepackage{amssymb}
\usepackage{amsthm}

\newtheorem{proposition}{Proposition}
\newtheorem{defin}{Definition}

\usepackage{hyperref}

\usepackage{tikz,pgfplots}
\usepackage{etoolbox}
%% This makes the colors annoyingly bright, but at least they're easy to distinguish.
\pgfplotsset{
  every  tick/.style={red,}, minor x tick num=1,
  cycle list={teal,every mark/.append style={fill=teal!80!black},mark=*\\%
orange,every mark/.append style={fill=orange!80!black},mark=square*\\%
cyan!60!black,every mark/.append style={fill=cyan!80!black},mark=otimes*\\%
red!70!white,mark=star\\%
lime!80!black,every mark/.append style={fill=lime},mark=diamond*\\%
red,densely dashed,every mark/.append style={solid,fill=red!80!black},mark=*\\%
yellow!60!black,densely dashed,
every mark/.append style={solid,fill=yellow!80!black},mark=square*\\%
black,every mark/.append style={solid,fill=gray},mark=otimes*\\%
blue,densely dashed,mark=star,every mark/.append style=solid\\%
red,densely dashed,every mark/.append style={solid,fill=red!80!black},mark=diamond*\\%
}
}
\pgfplotsset{compat=1.6}

\input{paralleltable.tex}
\input{serialtable.tex}

\usepackage{xcolor}
\newcommand{\citefont}[1]{{\tiny \textcolor{Gray}{#1}}}


\title{Cache-Efficient Parallel Partition Algorithms}
\author{Alek Westover}
\institute{MIT PRIMES}
\date{October 20, 2019}

\begin{document}
 
\frame{\titlepage}

\begin{frame}[t]{The Partition Problem}
	\onslide<1->{
		An unpartitioned array:\\
		\vspace{0.25cm}
		\includegraphics[width=\linewidth]{imgs/partitionDefn/partitionDefn1Ann.eps}\\
		\vspace{1.5cm}
	}
	\onslide<2->{
		An array partitioned relative to a pivot value:\\
		\vspace{0.25cm}
		\includegraphics[width=\linewidth]{imgs/partitionDefn/partitionDefn2Ann.eps}
	}
\end{frame}

% \begin{frame}[t]{The Partition Problem}
%   \begin{center}
%     \begin{figure}
%       \includegraphics[width=\linewidth]{imgs/partitionDefinitionExplanation.eps}
%     \end{figure}
%   \end{center}
% \end{frame}

% \begin{frame}[t]{The Partition Problem}
%   \begin{center}
%     \begin{figure}
%       \includegraphics[width=\linewidth]{imgs/partitionDefinitionExplanation2.eps}
%     \end{figure}
%   \end{center}
% \end{frame}

\begin{frame}[t]{What is a Parallel Algorithm?}

	\begin{columns}[T] % align columns
	\begin{column}{.45\textwidth}
	Fundamental primitive: \defn{Parallel for loop}
	\vspace{2cm}

        \textbf{Parallel-For $i$ from $1$ to $4$: } \\
        \hspace{1 cm} \textbf{Do }$X_i$

%	\begin{algorithmic}
%		\State \textbf{parallel-for} $i \in \{1,2,3,4\}$
%		\State \hskip0.7cm do $X_i$
%		\State \textbf{endparallel-for}
%\end{algorithmic}

	\end{column}
	\hfill
	\begin{column}{.60\textwidth}
	\begin{figure}
		\includegraphics[width=0.75\linewidth]{imgs/parallelForLoop/altParallelForLoop.eps}
	\end{figure}
	\end{column}
	\end{columns}

\end{frame}


\begin{frame}[t]{What is a Parallel Algorithm?}
	More complicated parallel structures can be made by combining parallel for loops and recursion.
	\begin{columns}[T] % align columns
	\begin{column}{.45\textwidth}
	\end{column}
	\hfill
	\begin{column}{.60\textwidth}
		\begin{figure}
			\includegraphics[width=0.8\linewidth]{imgs/parallelForLoop/altParallelForLoopComposition.eps}
		\end{figure}
	\end{column}
	\end{columns}
\end{frame}

\begin{frame}[t]{\defn{$T_p$}: Time to run on $p$ processors}
	\begin{columns}[T] % align columns
	\begin{column}{.60\textwidth}
		\begin{figure}
			\includegraphics[width=0.8\linewidth]{imgs/parallelForLoop/altParallelForLoopComposition.eps}
		\end{figure}
	\end{column}
	\hfill
	\begin{column}{.45\textwidth}
		Important extreme cases:\\
		\vspace{0.3cm}
		\defn{Work:} $T_1$, 
		\begin{itemize}
			\item time to run in serial
			\item "sum of all work"
		\end{itemize}
		\vspace{0.3cm}
		\defn{Span:} $T_\infty$,
		\begin{itemize}
			\item time to run on infinitely many processors,\\ 
			\item "height of the graph"
		\end{itemize}	
	\end{column}
	\end{columns}
\end{frame}



\begin{frame}[t]{Bounding $T_p$ with Work and Span}
% \begin{defin}[$T_p$]
%   Running time on $p$ processors. Note: $T_p \ge T_\infty, T_p \ge \frac{T_1}{p}$.
% \end{defin}	
% \begin{defin}[Work]
%   Running time on a single processor. $T_1 = \sum_i W_i$.
% \end{defin}	
% \begin{defin}[Span]
%   Running time on infinitely many processors. $T_\infty = \sum_i 1$.
% \end{defin}	
	\defn{Brent's Theorem:} \citefont{[Brent, 74]}
% \begin{theorem}[Brent's Theorem]
	% $$T_p = \sum_i \Big\lceil \frac{W_i}{p} \Big\rceil \le \sum_i \Big( \frac{W_i}{p}+1 \Big) = \frac{T_1}{p}+T_\infty.$$	
	$$T_p = \Theta\left(\frac{T_1}{p}+T_\infty\right)$$
% \end{theorem}
	$ $\\ \vspace{1cm}
	\textbf{Take away:} Work $T_1$ and span $T_\infty$ determine $T_p$.
\end{frame}


\begin{frame}[t]{The Standard Parallel Partition Algorithm}
\begin{table}[]
\begin{tabular}{ll}
	\emph{Step}                                              & \emph{Span} \\\\
Create filtered array                             & $O(1)$            \\\\
Compute prefix sums of filtered array & $O(\log n)$       \\\\
Use prefix sums to partition array                & $O(1)$           
\end{tabular}
\end{table}
\vspace{10 mm}
Total span: $O(\log n)$
\end{frame}

\begin{frame}[t]{The Problem}
	\onslide<1->{Why is the Standard Algorithm is slow in practice?}
	\begin{columns}[T] % align columns
	\begin{column}{.65\textwidth}
	\begin{itemize}
		\item<2-> Uses extra memory % not in place
		\item<3-> Makes multiple passes over array
	\end{itemize}
	\end{column}
	\begin{column}{.35\textwidth}
	\onslide<4->{\hspace{-1cm}$\Biggr\}\text{ "bad cache behavior"}$}
	\end{column}
	\end{columns}
	\vspace{0.4 cm}

	\onslide<5->{But fastest algorithms in practice lack theoretical guarantees}
	\begin{itemize}
		\item<6-> Lock-based and atomic-variable based algorithms
		\item<7-> The Strided Algorithm\\ \citefont{[Francis and Pannan, 92; Frias and Petit, 08]}\\ 
		\onslide<8->{No locks or atomic-variables,\\but no bound on span in general}
	\end{itemize}
	\onslide<9-> {
	\vspace{0.4cm}
	\textbf{Our Question:}
Can we create an algorithm with theoretical guarantees that is fast in practice?}
\end{frame}

% \begin{frame}[t]{Cache Efficiency}
%   Rough definition:\\
%   The number of passes over the input data. \\
%   \vspace{1cm}
%   A cache-efficient algorithm will make relatively few requests to load data from memory into cache where it can be manipulated. \\
%   \vspace{1cm}
%   Poor cache behavior will harm an algorithms performance in practice.
% \end{frame}

\begin{frame}[t]{Our Result}
	We created a randomized algorithm for the parallel partition problem: the \defn{Smoothed-Striding Algorithm}.\\
	\vspace{0.5cm}
	The Smoothed-Striding algorithm has...
	\begin{itemize}
		\item performance comparable to that of the Strided Algorithm
		\item polylogarithmic span like the Standard Algorithm
		\item theoretically optimal cache behavior 
	\end{itemize}
\end{frame}

% \begin{frame}[t]{The Smoothed-Striding Algorithm's performance}
%   \begin{figure}
%     \begin{center}
%       \CILKtable
%     \end{center}
%   \end{figure}
% \end{frame}

\begin{frame}[t]{Algorithm Comparison}
	\begin{columns}[T] % align columns
	\begin{column}{.45\textwidth}
		\textbf{Strided Algorithm}\\\citefont{[Francis and Pannan, 92; Frias and Petit, 08]}\\
		\vspace{0.25cm}
		\begin{itemize}
			\item Good cache behavior in practice\\\hfill
			\item Worst case span is $T_\infty \approx n$\\\hfill
			\item But: on random inputs span is $T_\infty = \tilde{O}(n^{2/3})$
		\end{itemize}
	\end{column}
	\hfill
	\begin{column}{.55\textwidth}
		\textbf{Smoothed-Striding Algorithm}\\\citefont{}\\
		\vspace{0.25cm}
		\begin{itemize}
			\item Provably optimal cache behavior\\\hfill
			\item Span is $O(\log n \log\log n)$ with high probability in $n$\\\hfill
			\item Uses randomization inside the algorithm to remove the need for randomized input
		\end{itemize}
	\end{column}
	\end{columns}
\end{frame}

\begin{frame}[t]{Speedup over Serial Partition: Smoothed-Striding Algorithm's performance}
	\begin{figure}
		\begin{center}
			\includegraphics[width=0.9\linewidth]{imgs/compiledGraph.eps}
		\end{center}
	\end{figure}
\end{frame}

% \begin{frame}[t]{Randomized Algorithms}
%   \defn{With high probability in $n$:}
%     $$1-\frac{1}{n^c}$$
%   Can be made arbitrarily close to $1$ by choice of $c$.
% \end{frame}

% \begin{frame}[t]{Memory Bandwidth Bound}
%   \begin{defin}[Cache miss]
%     A cache miss occurs when the algorithm must load a cache-line not stored in cache into cache.
%   \end{defin}	
%   \begin{itemize}
%     \item In-place $\implies$ spatial-locality  
%     \item Kuszmaul developed in-place parallel partition algorithm
%       \begin{itemize}
%         \item Outperforms standard out-of-place algorithm
%         \item Outperformed by more cache-efficient higher span algorithm
%       \end{itemize}
%     \item Experiments show Memory Bandwidth Bound (incurring too many cache misses) is the problem
%     \item Want Temporal and Spatial locality
%   \end{itemize}
% \end{frame}


% \begin{frame}[t]{The Serial Partition Algorithm}
% \begin{figure}
%     \begin{overprint}
%       \onslide<1>\includegraphics[width=\linewidth]{imgs/serialPartition/serialPartition1.eps}
%     \onslide<2>\includegraphics[width=\linewidth]{imgs/serialPartition/serialPartition2.eps}
%     \onslide<3>\includegraphics[width=\linewidth]{imgs/serialPartition/serialPartition3.eps}
%     \onslide<4->\includegraphics[width=\linewidth]{imgs/serialPartition/serialPartition4.eps}
%     \end{overprint}
%   \end{figure}	
% \end{frame}

\begin{frame}[t]{}
	\vfill
	\begin{center}
		{\Huge The Strided Algorithm}\\
		\citefont{[Francis and Pannan, 92; Frias and Petit, 08]}
	\end{center}
	\vfill
\end{frame}


%% BILL NOTES:
%% extra slide introducing v_i (v_4?...)
%% extra slide for partition vmin through vmax in serial 

% \begin{frame}[t]{}%{Strided Algorithm Description}
%   \vspace{0.25cm}
%   \begin{overprint}
%   \onslide<1>Logically partition the array into chunks of adjacent elements:
%   \onslide<2>Form groups $P_i$ where $P_i$ contains the $i$-th element from each chunk:
%   \onslide<3>Perform serial partitions on each $P_i$ in parallel over the $P_i$'s:
%   \onslide<4>Identify the splitting index $v_i$ (the first element greater than the pivot) of each $P_i$. 
%   \onslide<5>Partition the subarray from the minimum splitting index to the maximum splitting index in serial. This completes the partition. 
%   \end{overprint}
%   \vspace{0.25cm}
%   \begin{overprint}
%   \onslide<1>\includegraphics[width=\linewidth]{imgs/oldStridedAlgSim/stridedAlgSim1Ann.eps}
%   \onslide<2>\includegraphics[width=\linewidth]{imgs/oldStridedAlgSim/stridedAlgSim2Ann.eps}
%   \onslide<3>\includegraphics[width=\linewidth]{imgs/oldStridedAlgSim/stridedAlgSim3Ann.eps}
%   \onslide<4>\includegraphics[width=\linewidth]{imgs/oldStridedAlgSim/stridedAlgSim4Ann.eps}
%   \onslide<5>\includegraphics[width=\linewidth]{imgs/oldStridedAlgSim/stridedAlgSim5Ann.eps}
%   \end{overprint}
%   \vspace{0.25cm}
%   \begin{overprint}
%   \onslide<3>This step is highly parallel.
%   % \onslide<4>Note that all elements below the minimum splitting index are less than the pivot and all elements greater than the maximum splitting index are greater than the pivot.
%   \onslide<5> Note that this step has no parallelism. In general this results in span $O(n)$. However, if the number of elements less than the pivot in each $P_i$ is similar, then size of the subarray to be partitioned can be very small.
%   \end{overprint}
% \end{frame}

\begin{frame}[t]{}%{Strided Algorithm Description}
	\vspace{0.25cm}
	\begin{overprint}
	\onslide<1>Logically partition the array into chunks of adjacent elements:
	\onslide<2>Form groups $P_i$ where $P_i$ contains the $i$-th element from each chunk:
	\onslide<3>Perform serial partitions on each $P_i$ in parallel over the $P_i$'s:
	\onslide<5>Identify the splitting index $v_i$ (the first element greater than the pivot) of each $P_i$. 
	\onslide<7>Partition the subarray from the minimum splitting index to the maximum splitting index in serial. This completes the partition. 
	\end{overprint}
	\vspace{0.25cm}
	\begin{overprint}
	\onslide<1>\includegraphics[width=\linewidth]{imgs/stridedAlgSim/stridedAlgSim_1.eps}
	\onslide<2>\includegraphics[width=\linewidth]{imgs/stridedAlgSim/stridedAlgSim_2.eps}
	\onslide<3>\includegraphics[width=\linewidth]{imgs/stridedAlgSim/stridedAlgSim_3.eps}
	\onslide<4>\includegraphics[width=\linewidth]{imgs/stridedAlgSim/stridedAlgSim_35.eps}
	\onslide<5>\includegraphics[width=\linewidth]{imgs/stridedAlgSim/stridedAlgSim_4.eps}
	\onslide<6>\includegraphics[width=\linewidth]{imgs/stridedAlgSim/stridedAlgSim_45.eps}
	\onslide<7>\includegraphics[width=\linewidth]{imgs/stridedAlgSim/stridedAlgSim_5.eps}
	\end{overprint}
	\vspace{0.25cm}
	\begin{overprint}
	\onslide<3>This step is highly parallel.
	% \onslide<4>Note that all elements below the minimum splitting index are less than the pivot and all elements greater than the maximum splitting index are greater than the pivot.
	\onslide<7> Note that this step has no parallelism. In general this results in span $O(n)$. However, if the number of elements less than the pivot in each $P_i$ is similar, then size of the subarray to be partitioned can be very small.
	\end{overprint}
\end{frame}

% \begin{frame}[t]{Strided Algorithm Description [Francis and Pannan, 92; Frias and Petit, 08]}
%   % Partition $A$ into chunks $C_1,C_2,\ldots C_n/gb$ each consisting of $g$ cache lines of size $b$. Let $P_i$ be the union of the $i$-th cache-line from each chunk $C_j$.
%   \begin{figure}
%     \includegraphics[width=\linewidth]{imgs/stridedAlgArrows.eps}
%   \end{figure}
%   % \begin{defin}[Partially Partitioned Array]
%   %   $\exists u, l $ such that $$i < u \implies A[i] \text{ is predecessor }, i \ge l \implies A[i] \text{ is successor }$$
%   % \end{defin}
%   \begin{itemize}
%     \item Logically partition $A_i$ into $P_i$ so that $P_i$ has the $i$-th block from each chunk $C_j$ of the array
%     \item Perform serial partitions on all $P_i$ in parallel.
%     \item Let $v_i$ be the position in $A$ of the first successor in $P_i$. Perform a serial partition on $A[\min_i v_i], \ldots, A[\max_i v_i -1]$.
%   \end{itemize}
% \end{frame}


% \begin{frame}[t]{Strided Algorithm Analysis}
%   \begin{itemize}
%     \item Partial partition step: work $O(n)$, span $\Theta(n/g)$.
%     \item Serial cleanup step: span $\Theta(v_\text{max}-v_\text{min})$, which is $O(n)$ in general.
%     \item If the number of predecessors in each $P_i$ is similar, $v_\text{max}-v_\text{min}$ can be small.
%     \item If array values are selected independently at random from some distribution, for appropriate choice of parameters, with high probability in $n$, the Strided Algorithm achieves span $$\tilde{O}(n^{2/3}),$$ and the number of cache misses is fewer than $$\frac{n}{b}+\frac{\tilde{O}(n^{2/3})}{b}.$$
%     % \item In particular, if $b\in \polylog(n)$, and the array values are selected independently at random from some distribution, and $g$ is chosen to optimize span ($g=n^{1/3}$), then with high probability in $n$, $$v_\text{max}-v_\text{min} < \tilde{O}(n^{2/3}),$$ the span is $$\tilde{O}(n^{2/3}),$$ and the number of cache misses is fewer than $$\frac{n}{b}+\frac{\tilde{O}(n^{2/3})}{b}.$$
%   \end{itemize}
% \end{frame}

% \begin{frame}[t]{The Smoothed Striding Algorithm} 
%   The Smoothed-Striding Algorithm creates groups analogous to the Strided Algorithm's $P_i$'s, but rather than taking the $i$-th element from each chunk of the array to form groups $P_i$, the Smoothed-Striding algorithm takes a random element from each chunk for each of the groups $U_i$.

%   \textbf{Blocked Strided Algorithm $P_i$.}
%   \begin{figure}
%     \includegraphics[width=\linewidth]{imgs/stridedAlgHighlighted.png}
%   \end{figure}
%   \textbf{Smoothed-Striding Algorithm $U_i$.}
%   \begin{figure}
%     \includegraphics[width=\linewidth]{imgs/smoothedStridingAlgHighlighted.png}
%   \end{figure}
% \end{frame}


\begin{frame}[t]{}
	\vfill
	\begin{center}
		{\Huge The Smoothed-Striding Algorithm}
	\end{center}
	\vfill
\end{frame}

% \begin{frame}[t]{}%{Smoothed Striding Algorithm Description}
%   \vspace{0.25cm}
%   \begin{overprint}
%   \onslide<1>Logically partition the array into chunks of adjacent elements:
%   \onslide<2>Form groups $U_i$ where $U_i$ contains the $i$-th element from each chunk:
%   \onslide<3>Perform serial partitions on each $U_i$ in parallel over the $U_i$'s:
%   \onslide<4>Identify the splitting index $v_i$ (the first element greater than the pivot) of each $P_i$. 
%   \onslide<5>Recursively apply this algorithm to partition the subarray from the minimum splitting index to the maximum splitting index in serial. This completes the partition. 
%   \end{overprint}
%   \vspace{0.25cm}
%   \begin{overprint}
%   \onslide<1>\includegraphics[width=\linewidth]{imgs/smoothedAlgSim1Ann.eps}
%   \onslide<2>\includegraphics[width=\linewidth]{imgs/smoothedAlgSim2Ann.eps}
%   \onslide<3>\includegraphics[width=\linewidth]{imgs/smoothedAlgSim3Ann.eps}
%   \onslide<4>\includegraphics[width=\linewidth]{imgs/smoothedAlgSim4Ann.eps}
%   \onslide<5>\includegraphics[width=\linewidth]{imgs/smoothedAlgSim5Ann.eps}
%   \end{overprint}
%   \vspace{0.25cm}
%   \begin{overprint}
%   \onslide<3>This step is highly parallel.
%   % \onslide<4>Note that all elements below the minimum splitting index are less than the pivot and all elements greater than the maximum splitting index are greater than the pivot.
%   \onslide<5>Unlike in the Strided Algorithm this step has parallelism, and is guaranteed to only run on a small subarray. The Strided Algorithm could not recurse here because the subproblem is a worst case input for it.
%   \end{overprint}
% \end{frame}

\begin{frame}[t]{}%{Smoothed Striding Algorithm Description}
	\vspace{0.25cm}
	\begin{overprint}
	\onslide<1>Logically partition the array into chunks of adjacent elements:
	\onslide<2>Form groups $U_i$ where $U_i$ contains the $i$-th element from each chunk:
	\onslide<3>Perform serial partitions on each $U_i$ in parallel over the $U_i$'s:
	\onslide<4>Identify the splitting index $v_i$ (the first element greater than the pivot) of each $P_i$. 
	\onslide<5>Recursively apply this algorithm to partition the subarray from the minimum splitting index to the maximum splitting index in serial. This completes the partition. 
	\end{overprint}
	\vspace{0.25cm}
	\begin{overprint}
	\onslide<1>\includegraphics[width=\linewidth]{imgs/smoothedAlgSim/smoothedAlgSim_1.eps}
	\onslide<2>\includegraphics[width=\linewidth]{imgs/smoothedAlgSim/smoothedAlgSim_2.eps}
	\onslide<3>\includegraphics[width=\linewidth]{imgs/smoothedAlgSim/smoothedAlgSim_3.eps}
	\onslide<4>\includegraphics[width=\linewidth]{imgs/smoothedAlgSim/smoothedAlgSim_35.eps}
	\onslide<5>\includegraphics[width=\linewidth]{imgs/smoothedAlgSim/smoothedAlgSim_4.eps}
	\onslide<6>\includegraphics[width=\linewidth]{imgs/smoothedAlgSim/smoothedAlgSim_45.eps}
	\onslide<7>\includegraphics[width=\linewidth]{imgs/smoothedAlgSim/smoothedAlgSim_5.eps}
	\end{overprint}
	\vspace{0.25cm}
	\begin{overprint}
	\onslide<3>This step is highly parallel.
	% \onslide<4>Note that all elements below the minimum splitting index are less than the pivot and all elements greater than the maximum splitting index are greater than the pivot.
	\onslide<7>Unlike in the Strided Algorithm this step has parallelism, and is guaranteed to only run on a small subarray. The Strided Algorithm could not recurse here because the subproblem is a worst case input for it.
	\end{overprint}
\end{frame}

% \begin{frame}[t]{Technical Notes}
%   Storing the groups $U_i$ is a challenge. We can't explicitly store them because then the algorithm would not be in-place. By design they do not have a regular structure like the $P_i$ of the Strided Algorithm.\\
%   The solution is to store $U_1$ and then specify that all other $U_i$'s are a circular shift within the chunks of $U_1$.\\
%   More precisely, Let $X[1],\ldots, X[s]$ be chosen uniformly at random from $\{1,\ldots, g\}$. 
%   Then let $U_i$ be the union of the $(X[j]+i)\mod g$-th cache-line from each chunk $C_j$.\\
%   Note that the $U_i$'s are not indpendent, but this doesn't affect the union bound.\\
%   \vspace{0.5cm}
%   In order to compute the minimum and maxmium splitting indices $v_{\min}, v_{\max}$ in parallel we use a recursive structure rather than a parallel-for loop.
% \end{frame}

\begin{frame}[t]{A Key Challenge}
How do we store the $U_i$'s if they are all random?	\\
\vspace{0.5cm}
If we stored the parts that made up each $U_i$ we would not be in-place.\\
\vspace{0.5cm}
\textbf{Blocked Strided Algorithm $P_i$.}
\begin{figure}
	\includegraphics[width=\linewidth]{imgs/stridedAlgHighlighted.png}
\end{figure}
\textbf{Smoothed-Striding Algorithm $U_i$.}
\begin{figure}
	\includegraphics[width=\linewidth]{imgs/smoothedStridingAlgHighlighted.png}
\end{figure}
\end{frame}


% \begin{frame}[t]{Smoothed Striding Algorithm Description}
%   Let $X[1],\ldots, X[s]$ be chosen uniformly at random from $\{1,\ldots, g\}$. Let $U_i$ be the union of the $(X[j]+i)\mod g$-th cache-line from each chunk $C_j$.
%   \begin{figure}
%     \includegraphics[width=\linewidth]{imgs/smoothedStridingAlgHighlighted.png}
%   \end{figure}
%   \begin{itemize}
%     \item Perform serial partitions on all $U_i$ in parallel.
%     \item The array is partially now partitioned with $A[i]$ a predecessor for all $i < v_{\text{min}}$ and $A[i]$ a successor for all $i \ge v_{\text{max}}$.
%   \end{itemize}
%   Note that we will make $s = \frac{n}{gb} < \polylog(n)$ so the algorithm remains in-place.
% \end{frame}


% \begin{frame}[t]{Partial Partition Step Analysis}
% \begin{proposition}
%   %% The 1/2's are necessary for the final line of the proof to easily go through.
%   Let $\epsilon \in (0, 1/2)$ and $\delta \in (0, 1/2)$ such that
%   $\epsilon \ge \frac{1}{\poly(n)}$ and $\delta \ge
%   \frac{1}{\polylog(n)}$. Suppose $s > \frac{\ln
%     (n/\epsilon)}{\delta^2}$. Finally, suppose that each processor has
%   a cache of size at least $s + c$ for a sufficiently large constant
%   $c$.

%   Then the Partial-Partition Algorithm achieves work $O(n)$; achieves
%   span $O\paren*{b \cdot s}$; incurs $\frac{s+n}{b} + O(1)$ cache
%   misses; and guarantees with probability $1 - \epsilon$ that
%   $$v_{\text{max}}-v_{\text{min}} < 4 n \delta.$$
% \end{proposition}

% \end{frame}

% \begin{frame}[t]{From Partial Partition to Full Partition}
%   \emph{Partial Partition Step:}
%   \begin{itemize}
%     \item Use $\epsilon = 1/n^c$ for $c$ of our choice (i.e. with high probability).
%     \item Choice of $\delta$ results in tradeoff between cache misses and span.
%   \end{itemize}
% \end{frame}

% \begin{frame}[t]{From Partial Partition to Full Partition}
%   \emph{Recursive strategies:}
%   \begin{itemize}
%     \item \defn{Hybrid Smoothed Striding Algorithm}: Use algorithm with span $O(\log n \log \log n)$. Note: recursive algorithm's cache behavior doesn't affect overall cache behavior because subarray is small. This algorithm can be tuned to give optimal span and cache misses.
%     \item \defn{Recursive Smoothed Striding Algorithm}: Use the Partial Partition step recursively to solve subproblems. Recursive applications of the Partial Partition step use the same $\epsilon$ the top-level (to guarantee success with high probability in $n$), and use $\delta \in \Theta(1)$ such that the problem size is reduced by half at each step. This algorithm has slightly worse span, but is very simple to implement.
%   \end{itemize}
% \end{frame}

% \begin{frame}[t]{Hybrid Algorithm Analysis - General Theorem}
% \begin{theorem}
%   The Hybrid Smoothed Striding Algorithm algorithm using parameter $\delta\in(0,1/2)$ satisfying $\delta \ge 1/\polylog(n)$: has work $O(n)$; achieves span
%         $$O\paren*{\log n \log\log n +\frac{b\log n}{\delta^2}},$$
% with high probability in $n$; and incurs fewer than 
% $$(n+O(n\delta))/b$$
% cache misses with high probability in $n$.
% \end{theorem}
% \end{frame}

% \begin{frame}[t]{Hybrid Algorithm Analysis - Corollary for specific parameter settings}
% An interesting corollary of the above theorem concerns what happens when $b$ is small (e.g., constant) and we choose $\delta$ to optimize span. 
% \begin{corollary}
% Suppose $b \le o(\log \log n)$. Then the Cache-Efficient Full-Partition Algorithm algorithm using $\delta = \Theta\big(\sqrt{b/\log\log n}\big)$, achieves work $O(n)$, and with high probability in $n$, achieves span $O(\log n \log\log n)$ and incurs fewer than $(n+o(n))/b$ cache misses.
% \end{corollary}
% \end{frame}


% \begin{frame}[t]{Recursive Algorithm Analysis - General Theorem}
%   \begin{theorem}
%   With high probability in $n$, the Recursive Smoothed Striding
%         algorithm using parameter $\delta \in(0,1/2)$ satisfying
%         $\delta \ge 1 / \polylog(n)$: achieves work $O(n)$, attains span
%   $$O\left(b\left(\log^2 n + \frac{\log n}{\delta^2}\right)\right),$$
%   and incurs $(n+O(n \delta))/b$ cache misses. 
%   \end{theorem}
% \end{frame}

% \begin{frame}[t]{Recursive Algorithm Analysis - Corollary for specific parameter settings}
% A particularly natural parameter setting for the Recursive algorithm occurs at $\delta = 1 / \sqrt{\log n}$.
% \begin{corollary}
%   With high probability in $n$, the Recursive Smoothed Striding Algorithm using parameter $\delta=1/\sqrt{\log n}$:
%   achieves work $O(n)$, attains span $O(b\log^2 n)$, and incurs $n/b \cdot (1 + O(1 / \sqrt{\log n}))$ cache misses. 
% \end{corollary}
% \end{frame}


\begin{frame}[t]{Open Questions}
	By recursively applying the Smoothed-Striding algorithm we get an algorithm for
	parallel partition that incurs $n(1+o(1))$ cache misses and has span
	$O(b\log^2 n)$.\\
	\vspace{0.25cm}
	There are techniques for improving this span to $O(\log n\log\log n)$ while retaining the cache behavior.\\
	\vspace{0.25cm}
	But the standard algorithm has span $O(\log n)$.\\
	\vspace{1cm}
	Can we construct an algorithm that achieves optimal cache behavior and span $O(\log n)$?
\end{frame}

\begin{frame}[t]{Acknowledgments}
I would like to thank
\begin{itemize}
	\item {The MIT PRIMES program}
	\item {William Kuszmaul, my PRIMES mentor}
	\item {My parents}
\end{itemize}
\end{frame}

\end{document}
