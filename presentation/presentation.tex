%% question .... summary
%% :hard to guarantee that it will scale well
%% down down
%% Strdied Algorithm VS Smoothed striding algorith slide practice
%% UCLA: maybe add that rainbow slide (better gradient though), add pictures for questions after the acknowledgements
%% pivot value introduction

\documentclass[xcolor=x11names, svgnames, rgb]{beamer}

\setbeamertemplate{navigation symbols}{}
\setbeamercolor{block title}{bg=blue!40}
\setbeamercolor{block body}{bg=blue!20}

%% Beamer Layout %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\useoutertheme[subsection=false,shadow]{miniframes}
\useinnertheme{default}
\usefonttheme{serif}
\usepackage{palatino}
\setbeamerfont{title like}{shape=\scshape}
\setbeamerfont{frametitle}{shape=\scshape}
\setbeamercolor*{lower separation line head}{bg=DeepSkyBlue4}
\setbeamercolor*{normal text}{fg=black,bg=white}
\setbeamercolor*{alerted text}{fg=red}
\setbeamercolor*{example text}{fg=black}
\setbeamercolor*{structure}{fg=black}
\setbeamercolor*{palette tertiary}{fg=black,bg=black!10}
\setbeamercolor*{palette quaternary}{fg=black,bg=black!10}
%% END Beamer Layout %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{soul}

\usepackage{mathtools}
\newcommand{\defeq}{\vcentcolon=}
\DeclarePairedDelimiter{\paren}{(}{)}

\newcommand{\dec}{\operatorname{dec}}
\newcommand{\poly}{\operatorname{poly}}
\newcommand{\polylog}{\operatorname{polylog}}
\newcommand{\github}{\url{github.com/awestover/Parallel-Partition}}
\newcommand{\defn}[1]       {{\textit{\textbf{\boldmath #1}}}}
\newcommand{\paragraph}[1]{\vspace{0.09in}\noindent{\bf \boldmath #1.}} 
\usepackage{amsmath}
\def\E{\operatorname{\mathbb{E}}}
\usepackage{amssymb}
\usepackage{amsthm}

\newtheorem{proposition}{Proposition}
\newtheorem{defin}{Definition}

\usepackage{hyperref}

\usepackage{tikz,pgfplots}
\usepackage{etoolbox}
%% This makes the colors annoyingly bright, but at least they're easy to distinguish.
\pgfplotsset{
  every  tick/.style={red,}, minor x tick num=1,
  cycle list={teal,every mark/.append style={fill=teal!80!black},mark=*\\%
orange,every mark/.append style={fill=orange!80!black},mark=square*\\%
cyan!60!black,every mark/.append style={fill=cyan!80!black},mark=otimes*\\%
red!70!white,mark=star\\%
lime!80!black,every mark/.append style={fill=lime},mark=diamond*\\%
red,densely dashed,every mark/.append style={solid,fill=red!80!black},mark=*\\%
yellow!60!black,densely dashed,
every mark/.append style={solid,fill=yellow!80!black},mark=square*\\%
black,every mark/.append style={solid,fill=gray},mark=otimes*\\%
blue,densely dashed,mark=star,every mark/.append style=solid\\%
red,densely dashed,every mark/.append style={solid,fill=red!80!black},mark=diamond*\\%
}
}
\pgfplotsset{compat=1.6}

\input{paralleltable.tex}
\input{serialtable.tex}

\usepackage{xcolor}
\newcommand{\citefont}[1]{{\tiny \textcolor{Gray}{#1}}}


\title{Cache-Efficient Parallel Partition Algorithms}
\author{Alek Westover}
\institute{MIT PRIMES}
\date{October 20, 2019}

\begin{document}
 
\frame{\titlepage}

\begin{frame}[t]{The Partition Problem}
	\onslide<1->{
		An unpartitioned array:\\
		\vspace{0.25cm}
		\includegraphics[width=\linewidth]{imgs/partitionDefn/partitionDefn1Ann.eps}\\
		\vspace{1.5cm}
	}
	\onslide<2->{
		An array partitioned relative to a pivot value:\\
		\vspace{0.25cm}
		\includegraphics[width=\linewidth]{imgs/partitionDefn/partitionDefn2Ann.eps}
	}
\end{frame}

% \begin{frame}[t]{The Partition Problem}
%   \begin{center}
%     \begin{figure}
%       \includegraphics[width=\linewidth]{imgs/partitionDefinitionExplanation.eps}
%     \end{figure}
%   \end{center}
% \end{frame}

% \begin{frame}[t]{The Partition Problem}
%   \begin{center}
%     \begin{figure}
%       \includegraphics[width=\linewidth]{imgs/partitionDefinitionExplanation2.eps}
%     \end{figure}
%   \end{center}
% \end{frame}

\begin{frame}[t]{What is a Parallel Algorithm?}

	\begin{columns}[T] % align columns
	\begin{column}{.45\textwidth}
	Fundamental primitive: \defn{Parallel for loop}
	\vspace{2cm}

        \textbf{Parallel-For $i$ from $1$ to $4$: } \\
        \hspace{1 cm} \textbf{Do }$X_i$

%	\begin{algorithmic}
%		\State \textbf{parallel-for} $i \in \{1,2,3,4\}$
%		\State \hskip0.7cm do $X_i$
%		\State \textbf{endparallel-for}
%\end{algorithmic}

	\end{column}
	\hfill
	\begin{column}{.60\textwidth}
	\begin{figure}
		\includegraphics[width=0.75\linewidth]{imgs/parallelForLoop/altParallelForLoop.eps}
	\end{figure}
	\end{column}
	\end{columns}

\end{frame}


\begin{frame}[t]{What is a Parallel Algorithm?}
	More complicated parallel structures can be made by combining parallel for loops and recursion.
	\begin{columns}[T] % align columns
	\begin{column}{.45\textwidth}
	\end{column}
	\hfill
	\begin{column}{.60\textwidth}
		\begin{figure}
			\includegraphics[width=0.8\linewidth]{imgs/parallelForLoop/altParallelForLoopComposition.eps}
		\end{figure}
	\end{column}
	\end{columns}
\end{frame}

\begin{frame}[t]{\defn{$T_p$}: Time to run on $p$ processors}
	\begin{columns}[T] % align columns
	\begin{column}{.60\textwidth}
		\begin{figure}
			\includegraphics[width=0.8\linewidth]{imgs/parallelForLoop/altParallelForLoopComposition.eps}
		\end{figure}
	\end{column}
	\hfill
	\begin{column}{.45\textwidth}
		Important extreme cases:\\
		\vspace{0.3cm}
		\defn{Work:} $T_1$
		\begin{itemize}
			\item time to run in serial
			\item "sum of all work"
		\end{itemize}
		\vspace{0.3cm}
		\defn{Span:} $T_\infty$
		\begin{itemize}
			\item time to run on infinitely many processors\\ 
			\item "height of the graph"
		\end{itemize}	
	\end{column}
	\end{columns}
\end{frame}



\begin{frame}[t]{Bounding $T_p$ with Work and Span}
% \begin{defin}[$T_p$]
%   Running time on $p$ processors. Note: $T_p \ge T_\infty, T_p \ge \frac{T_1}{p}$.
% \end{defin}	
% \begin{defin}[Work]
%   Running time on a single processor. $T_1 = \sum_i W_i$.
% \end{defin}	
% \begin{defin}[Span]
%   Running time on infinitely many processors. $T_\infty = \sum_i 1$.
% \end{defin}	
	\defn{Brent's Theorem:} \citefont{[Brent, 74]}
% \begin{theorem}[Brent's Theorem]
	% $$T_p = \sum_i \Big\lceil \frac{W_i}{p} \Big\rceil \le \sum_i \Big( \frac{W_i}{p}+1 \Big) = \frac{T_1}{p}+T_\infty.$$	
	$$T_p = \Theta\left(\frac{T_1}{p}+T_\infty\right)$$
% \end{theorem}
	$ $\\ \vspace{1cm}
	\textbf{Take away:} Work $T_1$ and span $T_\infty$ determine $T_p$.
\end{frame}


\begin{frame}[t]{The Standard Parallel Partition Algorithm}
\begin{table}[]
\begin{tabular}{ll}
	\emph{Step}                                              & \emph{Span} \\\\
Create filtered array                             & $O(1)$            \\\\
Compute prefix sums of filtered array & $O(\log n)$       \\\\
Use prefix sums to partition array                & $O(1)$           
\end{tabular}
\end{table}
\vspace{10 mm}
Total work: $T_1 = O(n)$\\
Total span: $T_\infty = O(\log n)$

\end{frame}

% \begin{frame}[t]{An In-Place verson of the Standard Algorithm {\color{red} add for Yau, cut for PRIMES?}}
%   In recent work Kuszmaul developed an in-place variant of the standard algorithm.\\
%   The algorithm has span $O(\log n \log \log n)$ and work $O(n)$.\\
%   The algorithm outperforms the standard algorithm but still does not scale well. \\
%   The reason is the memory bandwidth bound: \textbf{show picture}, follow up
%   later with the picture that has a line for memory bandwidth bound for
%   smoothed striding algorithm and kuszmaul's algorithm.
% \end{frame}

\begin{frame}[t]{The Problem}
	Standard Algorithm is slow in practice
	\begin{columns}[T] % align columns
	\begin{column}{.65\textwidth}
	\begin{itemize}
		\item Uses extra memory % not in place
		\item Makes multiple passes over array
	\end{itemize}
	\end{column}
	\begin{column}{.35\textwidth}
	\hspace{-1cm}$\Biggr\}\text{ "bad cache behavior"}$
	\end{column}
	\end{columns}
	\vspace{0.4 cm}

 Fastest algorithms in practice lack theoretical guarantees
	\begin{itemize}
		\item Lock-based and atomic-variable based algorithms\\ \citefont{[Michael Axtmann, Sascha Witt, Daniel Ferizovic, and Peter Sanders, 2017; Philip Heidelberger, Alan Norton, and John T. Robinson, 1990; Philippas Tsigas and Yi Zhang, 2003]}
		\item The Strided Algorithm\\ \citefont{[Francis and Pannan, 92; Frias and Petit, 08]}\\ 
		No locks or atomic-variables, but no bound on span
	\end{itemize}
	\vspace{0.2cm}
%   \textbf{Our Question:}
% Can we create an algorithm with theoretical guarantees that is fast in practice?
\end{frame}

\begin{frame}[t]{Our Question}
	\vspace{1cm}
	{\Large Can we create an algorithm with \emph{theoretical guarantees} that is \emph{fast in practice}?}
\end{frame}

% \begin{frame}[t]{Cache Efficiency}
%   Rough definition:\\
%   The number of passes over the input data. \\
%   \vspace{1cm}
%   A cache-efficient algorithm will make relatively few requests to load data from memory into cache where it can be manipulated. \\
%   \vspace{1cm}
%   Poor cache behavior will harm an algorithms performance in practice.
% \end{frame}

\begin{frame}[t]{Our Result}
	\vspace{0.15cm}
	{\Large The Smoothed-Striding Algorithm\\}
	\vspace{0.5cm}
	Key Features:
	\begin{itemize}
		\item linear work and polylogarithmic span \\
			{\color{blue} (like the Standard Algorithm)\\}
		\vspace{0.15cm}
		\item fast in practice \\
			{\color{blue} (like the Strided Algorithm)\\}
	\vspace{0.15cm}
		\item theoretically optimal cache behavior \\
			{\color{blue} (unlike any past algorithm)}
	\end{itemize}
\end{frame}

% \begin{frame}[t]{The Smoothed-Striding Algorithm's performance}
%   \begin{figure}
%     \begin{center}
%       \CILKtable
%     \end{center}
%   \end{figure}
% \end{frame}

\begin{frame}[t]{Strided Versus Smoothed-Striding Algorithm}
	\begin{columns}[T] % align columns
	\begin{column}{.45\textwidth}
		\onslide<1->{
		\textbf{Strided Algorithm}\\\citefont{[Francis and Pannan, 92; Frias and Petit, 08]}\\
		\vspace{0.25cm}
		\begin{itemize}
			\item Good cache behavior in practice\\\hfill
			\item Worst case span is $T_\infty \approx n$\\\hfill\\\hfill
			\item On random inputs span is $T_\infty = \tilde{O}(n^{2/3})$
		\end{itemize}
	}
	\end{column}
	\hfill
	\begin{column}{.55\textwidth}
		\onslide<2->{
		\textbf{Smoothed-Striding Algorithm}\\\citefont{}\\
		\vspace{0.25cm}
		\begin{itemize}
			\item Provably optimal cache behavior\\\hfill
			\item Span is \\$T_\infty = O(\log n \log\log n)$\\ with high probability in $n$\\\hfill
			\item Uses randomization \emph{inside} the algorithm % both about the recursion step and the no worst case inputs thing
		\end{itemize}
	}
	\end{column}
	\end{columns}
\end{frame}

\begin{frame}[t]{Smoothed-Striding Algorithm's performance}
	\begin{figure}
		\begin{center}
			\includegraphics[width=0.9\linewidth]{imgs/compiledGraph.eps}
		\end{center}
	\end{figure}
\end{frame}

% \begin{frame}[t]{Randomized Algorithms}
%   \defn{With high probability in $n$:}
%     $$1-\frac{1}{n^c}$$
%   Can be made arbitrarily close to $1$ by choice of $c$.
% \end{frame}

% \begin{frame}[t]{Memory Bandwidth Bound}
%   \begin{defin}[Cache miss]
%     A cache miss occurs when the algorithm must load a cache-line not stored in cache into cache.
%   \end{defin}	
%   \begin{itemize}
%     \item In-place $\implies$ spatial-locality  
%     \item Kuszmaul developed in-place parallel partition algorithm
%       \begin{itemize}
%         \item Outperforms standard out-of-place algorithm
%         \item Outperformed by more cache-efficient higher span algorithm
%       \end{itemize}
%     \item Experiments show Memory Bandwidth Bound (incurring too many cache misses) is the problem
%     \item Want Temporal and Spatial locality
%   \end{itemize}
% \end{frame}


\begin{frame}[t]{}
	\vfill
	\begin{center}
		{\Huge The Strided Algorithm}\\
		\citefont{[Francis and Pannan, 92; Frias and Petit, 08]}
	\end{center}
	\vfill
\end{frame}



\begin{frame}[t]{}%{Strided Algorithm Description}
	\vspace{0.25cm}
	\begin{overprint}
	\onslide<1>Logically partition the array into chunks of adjacent elements
	\onslide<2>Form groups $P_i$ that contain the $i$-th element from each chunk
	\onslide<3>Perform serial partitions on each $P_i$ in parallel over the $P_i$'s
	\onslide<4>Define $v_i=$ index of first element greater than the pivot in $P_i$
	\onslide<5>Identify leftmost and rightmost $v_i$
	\onslide<6> \textbf{Final step:} Recursively partition the subarray
	\onslide<7> \st{\textbf{Final step:} Recursively partition the subarray}  
	\onslide<8> \st{\textbf{Final step:} Recursively partition the subarray}  
	\end{overprint}
	\vspace{0.25cm}
	\begin{overprint}
	\onslide<1>\includegraphics[width=\linewidth]{imgs/stridedAlgSim/sim1.eps}
	\onslide<2>\includegraphics[width=\linewidth]{imgs/stridedAlgSim/sim2.eps}
	\onslide<3>\includegraphics[width=\linewidth]{imgs/stridedAlgSim/sim3.eps}
	\onslide<4>\includegraphics[width=\linewidth]{imgs/stridedAlgSim/sim35.eps}
	\onslide<5>\includegraphics[width=\linewidth]{imgs/stridedAlgSim/sim4.eps}
	\onslide<6>\includegraphics[width=\linewidth]{imgs/stridedAlgSim/sim45.eps}
	\onslide<7>\includegraphics[width=\linewidth]{imgs/stridedAlgSim/sim45.eps}
	\onslide<8>\includegraphics[width=\linewidth]{imgs/stridedAlgSim/sim45.eps}
	\end{overprint}
	\vspace{0.25cm}
	\begin{overprint}
	\onslide<3>This step is highly parallel.
	% \onslide<4>Note that all elements below the minimum splitting index are less than the pivot and all elements greater than the maximum splitting index are greater than the pivot.
\onslide<7> \begin{itemize} \item Recursion is impossible!  \item \textbf{Final Step: }Partition the subarray \emph{in serial}.  \end{itemize} \vspace{1cm} Subproblem Span $T_\infty \approx v_{\max} - v_{\min}$
\onslide<8> \begin{itemize} \item Recursion is impossible!  \item \textbf{Final Step: }Partition the subarray \emph{in serial}.  \end{itemize} \vspace{1cm} Subproblem Span $T_\infty \approx v_{\max} - v_{\min}$ {\color{blue} $\longleftarrow \,\, n $ in worst case.} % but Intuitively v_max - v_min should be small
	\end{overprint}
\end{frame}


\begin{frame}[t]{}
	\vfill
	\begin{center}
		{\Huge The Smoothed-Striding Algorithm}
	\end{center}
	\vfill
\end{frame}


%% possibly make a terrbile pun joke "get it parallelism"
\begin{frame}[t]{}%{Smoothed Striding Algorithm Description}
	\vspace{0.25cm}
	\begin{overprint}
	\onslide<1>Logically partition the array into chunks of adjacent elements
	\onslide<2>\textbf{Key difference:} Form groups $U_i$ that contain a random element from each chunk %(rather than containing the $i$-th element from each chunk like the Strided Algorithm's $P_i$'s) 
	\onslide<3>Perform serial partitions on each $U_i$ in parallel over the $U_i$'s
	\onslide<4>Define $v_i = $ index of first element greater than the pivot in $U_i$
	\onslide<5>Identify leftmost and rightmost $v_i$
	\onslide<6>\textbf{Final step:} Recursively partition the subarray
	\onslide<7>\textbf{Final step:} Recursively partition the subarray
	\end{overprint}
	\vspace{0.25cm}
	\begin{overprint}
	\onslide<1>\includegraphics[width=\linewidth]{imgs/smoothedStridingAlgSim/sim1.eps}
	\onslide<2>\includegraphics[width=\linewidth]{imgs/smoothedStridingAlgSim/sim2.eps}
	\onslide<3>\includegraphics[width=\linewidth]{imgs/smoothedStridingAlgSim/sim3.eps}
	\onslide<4>\includegraphics[width=\linewidth]{imgs/smoothedStridingAlgSim/sim35.eps}
	\onslide<5>\includegraphics[width=\linewidth]{imgs/smoothedStridingAlgSim/sim4.eps}
	\onslide<6>\includegraphics[width=\linewidth]{imgs/smoothedStridingAlgSim/sim45.eps}
	\onslide<7>\includegraphics[width=\linewidth]{imgs/smoothedStridingAlgSim/sim45.eps}
	% \onslide<8>\includegraphics[width=\linewidth]{imgs/smoothedAlgSim/smoothedAlgSim_45.eps}
	% \onslide<9>\includegraphics[width=\linewidth]{imgs/smoothedAlgSim/smoothedAlgSim_5.eps}
	\end{overprint}
	\vspace{0.25cm}
	\begin{overprint}
	\onslide<3>This step is highly parallel.
	% \onslide<4>Note that all elements below the minimum splitting index are less than the pivot and all elements greater than the maximum splitting index are greater than the pivot.
	% \onslide<7>Unlike in the Strided Algorithm this step has parallelism, and is guaranteed to only run on a small subarray. The Strided Algorithm could not recurse here because the subproblem is a worst case input for it.
\onslide<7> \begin{itemize} \item Recursion is now possible! \item Randomness guarantees that $v_{\max} - v_{\min}$ is small \end{itemize} 
% \onslide<8> \begin{itemize} \item Recursion is now possible!  \end{itemize} \vspace{1cm} Subproblem can be solved with an In-Place algorithm that has span $O(\log n \log \log n)$. {\color{blue} Theoretical guarantees for all inputs.}
	\end{overprint}
\end{frame}

% \begin{frame}[t]{Technical Notes}
%   Storing the groups $U_i$ is a challenge. We can't explicitly store them because then the algorithm would not be in-place. By design they do not have a regular structure like the $P_i$ of the Strided Algorithm.\\
%   The solution is to store $U_1$ and then specify that all other $U_i$'s are a circular shift within the chunks of $U_1$.\\
%   More precisely, Let $X[1],\ldots, X[s]$ be chosen uniformly at random from $\{1,\ldots, g\}$. 
%   Then let $U_i$ be the union of the $(X[j]+i)\mod g$-th cache-line from each chunk $C_j$.\\
%   Note that the $U_i$'s are not indpendent, but this doesn't affect the union bound.\\
%   \vspace{0.5cm}
%   In order to compute the minimum and maxmium splitting indices $v_{\min}, v_{\max}$ in parallel we use a recursive structure rather than a parallel-for loop.
% \end{frame}

% \begin{frame}[t]{An Important Technical Note {\color{red} Add for Yau, cut for PRIMES?}}
%   The algorithm as described actually has poor cache behavior. \\
%   The real algorithm that we made opperates on blocks of elements rather than
%   single elements, which vastly improves its cahce behavior, making it so that
%   we only have to access each cache-line about once rather than accessing each
%   element about once. 
% \end{frame}

\begin{frame}[t]{A Key Challenge}
How do we store the $U_i$'s if they are all random?	\\
\vspace{0.5cm}
Storing which elements make up each $U_i$ takes too much space!\\

\vspace{0.5cm}
\textbf{Strided Algorithm $P_i$.}
\begin{figure}
	\includegraphics[width=\linewidth]{imgs/stridedAlgHighlighted.png}
\end{figure}
\textbf{Smoothed-Striding Algorithm $U_i$.}
\begin{figure}
	\includegraphics[width=\linewidth]{imgs/smoothedStridingAlgHighlighted.png}
\end{figure}
\end{frame}


% \begin{frame}[t]{Smoothed Striding Algorithm Description}
%   Let $X[1],\ldots, X[s]$ be chosen uniformly at random from $\{1,\ldots, g\}$. Let $U_i$ be the union of the $(X[j]+i)\mod g$-th cache-line from each chunk $C_j$.
%   \begin{figure}
%     \includegraphics[width=\linewidth]{imgs/smoothedStridingAlgHighlighted.png}
%   \end{figure}
%   \begin{itemize}
%     \item Perform serial partitions on all $U_i$ in parallel.
%     \item The array is partially now partitioned with $A[i]$ a predecessor for all $i < v_{\text{min}}$ and $A[i]$ a successor for all $i \ge v_{\text{max}}$.
%   \end{itemize}
%   Note that we will make $s = \frac{n}{gb} < \polylog(n)$ so the algorithm remains in-place.
% \end{frame}


\begin{frame}[t]{An Open Question}
	\vspace{0.5cm}
	\textbf{Our algorithm:} span $T_\infty = O(\log n \log\log n)$\\
	\vspace{0.5cm}
	\textbf{Standard Algorithm:} span $T_\infty = O(\log n)$.\\
	\vspace{1cm}
	Can we get optimal cache behavior and span $O(\log n)$?
\end{frame}

\begin{frame}[t]{Acknowledgments}
\begin{itemize}
	\item {MIT PRIMES}
	\item {William Kuszmaul, my PRIMES mentor}
	\item {My parents}
\end{itemize}
\end{frame}


\begin{frame}[t]{}
	\vfill
	\begin{center}
		{\Huge Question Slides}
	\end{center}
	\vfill
\end{frame}

\begin{frame}[t]{How to Store the groups}
	The solution is to make the groups dependent on one another.\\
	Let $g$ be the size of a chunk.
	Then we only need to store a single group and then the elements of the other groups are determined by this group.\\
	Specifically, let $X$ be an array with values chosen uniformly from $\{1,2,\ldots,g\}$. Then the $i$-th element of $U_j$ has index 
	$$ 1 + ((X[i]+j)\mod g)$$
	\begin{figure}
		\includegraphics[width=\linewidth]{imgs/blackrainbow.png}
	\end{figure}	
\end{frame}

\begin{frame}[t]{The Serial Partition Algorithm}
\begin{figure}
		\begin{overprint}
			\onslide<1>\includegraphics[width=\linewidth]{imgs/serialPartition/serialPartition1.eps}
		\onslide<2>\includegraphics[width=\linewidth]{imgs/serialPartition/serialPartition2.eps}
		\onslide<3>\includegraphics[width=\linewidth]{imgs/serialPartition/serialPartition3.eps}
		\onslide<4->\includegraphics[width=\linewidth]{imgs/serialPartition/serialPartition4.eps}
		\end{overprint}
	\end{figure}	
\end{frame}

\begin{frame}[t]{The Standard Parallel Partition Algorithm }
	\begin{figure}
		\includegraphics[width=\linewidth]{imgs/standardAlg/standardAlg1.eps}
	\end{figure}
\end{frame}

\begin{frame}[t]{The Standard Parallel Partition Algorithm}
	\begin{columns}[T] % align columns
	\begin{column}{.5\textwidth}
		\includegraphics[width=\linewidth]{imgs/standardAlg/standardAlg2.eps}
	\end{column}
	\begin{column}{.5\textwidth}
		\includegraphics[width=\linewidth]{imgs/standardAlg/standardAlg3.eps}
	\end{column}
	\end{columns}
\end{frame}

\end{document}
